{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ie2a96qOF29q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Dataset/property_listing_data_in_Bangladesh.csv')\n",
        "data.head(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 988
        },
        "id": "rn3YdowMbOAC",
        "outputId": "2285d5d3-5484-4a57-9435-4ad695ac02e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                title beds bath        area  \\\n",
              "0   Eminent Apartment Of 2200 Sq Ft Is Vacant For ...   3    4   2,200 sqft   \n",
              "1   Apartment Ready To Rent In South Khulshi, Near...   3    4   1,400 sqft   \n",
              "2   Smartly priced 1950 SQ FT apartment, that you ...   3    4   1,950 sqft   \n",
              "3   2000 Sq Ft Residential Apartment Is Up For Ren...   3    3   2,000 sqft   \n",
              "4   Strongly Structured This 1650 Sq. ft Apartment...   3    4   1,650 sqft   \n",
              "..                                                ...  ...  ...         ...   \n",
              "95  1350 Sq Ft Road Sided Apartment For Rent In Na...   3    3   1,350 sqft   \n",
              "96  A Suitable Flat Of 1100 Sq Ft With 3 Bedrooms ...   3    3   1,100 sqft   \n",
              "97  In An Urban Location And Reasonable Price, Thi...   3    3   2,000 sqft   \n",
              "98  A 2000 Square Feet Beautiful Apartment Is Read...   3    3   2,000 sqft   \n",
              "99  New With A View – 2000 Sq Ft Apartment For Ren...   3    3   2,000 sqft   \n",
              "\n",
              "                                               adress       type   purpose  \\\n",
              "0                     Block A, Bashundhara R-A, Dhaka  Apartment  For Rent   \n",
              "1                  South Khulshi, Khulshi, Chattogram  Apartment  For Rent   \n",
              "2                     Block F, Bashundhara R-A, Dhaka  Apartment  For Rent   \n",
              "3                             Sector 9, Uttara, Dhaka  Apartment  For Rent   \n",
              "4                     Block I, Bashundhara R-A, Dhaka  Apartment  For Rent   \n",
              "..                                                ...        ...       ...   \n",
              "95  Nasirabad Properties Residential Area, Khulshi...  Apartment  For Rent   \n",
              "96                       East Kazipara, Mirpur, Dhaka  Apartment  For Rent   \n",
              "97  VIP Housing Society, South Khulshi, Khulshi, C...  Apartment  For Rent   \n",
              "98  VIP Housing Society, South Khulshi, Khulshi, C...  Apartment  For Rent   \n",
              "99  VIP Housing Society, South Khulshi, Khulshi, C...  Apartment  For Rent   \n",
              "\n",
              "                                             flooPlan  \\\n",
              "0   https://images-cdn.bproperty.com/thumbnails/10...   \n",
              "1   https://images-cdn.bproperty.com/thumbnails/44...   \n",
              "2   https://images-cdn.bproperty.com/thumbnails/11...   \n",
              "3   https://images-cdn.bproperty.com/thumbnails/14...   \n",
              "4   https://images-cdn.bproperty.com/thumbnails/10...   \n",
              "..                                                ...   \n",
              "95  https://images-cdn.bproperty.com/thumbnails/14...   \n",
              "96  https://images-cdn.bproperty.com/thumbnails/15...   \n",
              "97  https://images-cdn.bproperty.com/thumbnails/15...   \n",
              "98  https://images-cdn.bproperty.com/thumbnails/32...   \n",
              "99  https://images-cdn.bproperty.com/thumbnails/13...   \n",
              "\n",
              "                                                  url        lastUpdated  \\\n",
              "0   https://www.bproperty.com/en/property/details-...    August 13, 2022   \n",
              "1   https://www.bproperty.com/en/property/details-...   January 25, 2022   \n",
              "2   https://www.bproperty.com/en/property/details-...  February 22, 2023   \n",
              "3   https://www.bproperty.com/en/property/details-...   October 28, 2021   \n",
              "4   https://www.bproperty.com/en/property/details-...  February 19, 2023   \n",
              "..                                                ...                ...   \n",
              "95  https://www.bproperty.com/en/property/details-...       June 4, 2022   \n",
              "96  https://www.bproperty.com/en/property/details-...   February 3, 2022   \n",
              "97  https://www.bproperty.com/en/property/details-...   February 2, 2022   \n",
              "98  https://www.bproperty.com/en/property/details-...   February 2, 2022   \n",
              "99  https://www.bproperty.com/en/property/details-...   February 2, 2022   \n",
              "\n",
              "          price  \n",
              "0   50 Thousand  \n",
              "1   30 Thousand  \n",
              "2   30 Thousand  \n",
              "3   35 Thousand  \n",
              "4   25 Thousand  \n",
              "..          ...  \n",
              "95  23 Thousand  \n",
              "96  16 Thousand  \n",
              "97  35 Thousand  \n",
              "98  35 Thousand  \n",
              "99  35 Thousand  \n",
              "\n",
              "[100 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9ac5c2a4-c784-4d4c-a9e4-40f6a8e79416\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>area</th>\n",
              "      <th>adress</th>\n",
              "      <th>type</th>\n",
              "      <th>purpose</th>\n",
              "      <th>flooPlan</th>\n",
              "      <th>url</th>\n",
              "      <th>lastUpdated</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Eminent Apartment Of 2200 Sq Ft Is Vacant For ...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2,200 sqft</td>\n",
              "      <td>Block A, Bashundhara R-A, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/10...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>August 13, 2022</td>\n",
              "      <td>50 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Apartment Ready To Rent In South Khulshi, Near...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,400 sqft</td>\n",
              "      <td>South Khulshi, Khulshi, Chattogram</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/44...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>January 25, 2022</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Smartly priced 1950 SQ FT apartment, that you ...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,950 sqft</td>\n",
              "      <td>Block F, Bashundhara R-A, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/11...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>February 22, 2023</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2000 Sq Ft Residential Apartment Is Up For Ren...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2,000 sqft</td>\n",
              "      <td>Sector 9, Uttara, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/14...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>October 28, 2021</td>\n",
              "      <td>35 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Strongly Structured This 1650 Sq. ft Apartment...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,650 sqft</td>\n",
              "      <td>Block I, Bashundhara R-A, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/10...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>February 19, 2023</td>\n",
              "      <td>25 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>1350 Sq Ft Road Sided Apartment For Rent In Na...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1,350 sqft</td>\n",
              "      <td>Nasirabad Properties Residential Area, Khulshi...</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/14...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>June 4, 2022</td>\n",
              "      <td>23 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>A Suitable Flat Of 1100 Sq Ft With 3 Bedrooms ...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1,100 sqft</td>\n",
              "      <td>East Kazipara, Mirpur, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/15...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>February 3, 2022</td>\n",
              "      <td>16 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>In An Urban Location And Reasonable Price, Thi...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2,000 sqft</td>\n",
              "      <td>VIP Housing Society, South Khulshi, Khulshi, C...</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/15...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>February 2, 2022</td>\n",
              "      <td>35 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>A 2000 Square Feet Beautiful Apartment Is Read...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2,000 sqft</td>\n",
              "      <td>VIP Housing Society, South Khulshi, Khulshi, C...</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/32...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>February 2, 2022</td>\n",
              "      <td>35 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>New With A View – 2000 Sq Ft Apartment For Ren...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2,000 sqft</td>\n",
              "      <td>VIP Housing Society, South Khulshi, Khulshi, C...</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/13...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>February 2, 2022</td>\n",
              "      <td>35 Thousand</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ac5c2a4-c784-4d4c-a9e4-40f6a8e79416')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9ac5c2a4-c784-4d4c-a9e4-40f6a8e79416 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9ac5c2a4-c784-4d4c-a9e4-40f6a8e79416');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(data[(data[\"type\"] == \"Building\") | (data[\"type\"] == \"Duplex\")].index)\n",
        "data.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "74R7w68_ijru",
        "outputId": "5a335c8f-f2bf-4796-d402-3231b5f7ed2b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title beds bath        area  \\\n",
              "0  Eminent Apartment Of 2200 Sq Ft Is Vacant For ...   3    4   2,200 sqft   \n",
              "1  Apartment Ready To Rent In South Khulshi, Near...   3    4   1,400 sqft   \n",
              "2  Smartly priced 1950 SQ FT apartment, that you ...   3    4   1,950 sqft   \n",
              "3  2000 Sq Ft Residential Apartment Is Up For Ren...   3    3   2,000 sqft   \n",
              "4  Strongly Structured This 1650 Sq. ft Apartment...   3    4   1,650 sqft   \n",
              "\n",
              "                               adress       type   purpose  \\\n",
              "0     Block A, Bashundhara R-A, Dhaka  Apartment  For Rent   \n",
              "1  South Khulshi, Khulshi, Chattogram  Apartment  For Rent   \n",
              "2     Block F, Bashundhara R-A, Dhaka  Apartment  For Rent   \n",
              "3             Sector 9, Uttara, Dhaka  Apartment  For Rent   \n",
              "4     Block I, Bashundhara R-A, Dhaka  Apartment  For Rent   \n",
              "\n",
              "                                            flooPlan  \\\n",
              "0  https://images-cdn.bproperty.com/thumbnails/10...   \n",
              "1  https://images-cdn.bproperty.com/thumbnails/44...   \n",
              "2  https://images-cdn.bproperty.com/thumbnails/11...   \n",
              "3  https://images-cdn.bproperty.com/thumbnails/14...   \n",
              "4  https://images-cdn.bproperty.com/thumbnails/10...   \n",
              "\n",
              "                                                 url        lastUpdated  \\\n",
              "0  https://www.bproperty.com/en/property/details-...    August 13, 2022   \n",
              "1  https://www.bproperty.com/en/property/details-...   January 25, 2022   \n",
              "2  https://www.bproperty.com/en/property/details-...  February 22, 2023   \n",
              "3  https://www.bproperty.com/en/property/details-...   October 28, 2021   \n",
              "4  https://www.bproperty.com/en/property/details-...  February 19, 2023   \n",
              "\n",
              "         price  \n",
              "0  50 Thousand  \n",
              "1  30 Thousand  \n",
              "2  30 Thousand  \n",
              "3  35 Thousand  \n",
              "4  25 Thousand  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5a87d2ad-a263-46c3-8ea5-dbe5d32a37b3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>area</th>\n",
              "      <th>adress</th>\n",
              "      <th>type</th>\n",
              "      <th>purpose</th>\n",
              "      <th>flooPlan</th>\n",
              "      <th>url</th>\n",
              "      <th>lastUpdated</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Eminent Apartment Of 2200 Sq Ft Is Vacant For ...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2,200 sqft</td>\n",
              "      <td>Block A, Bashundhara R-A, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/10...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>August 13, 2022</td>\n",
              "      <td>50 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Apartment Ready To Rent In South Khulshi, Near...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,400 sqft</td>\n",
              "      <td>South Khulshi, Khulshi, Chattogram</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/44...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>January 25, 2022</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Smartly priced 1950 SQ FT apartment, that you ...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,950 sqft</td>\n",
              "      <td>Block F, Bashundhara R-A, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/11...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>February 22, 2023</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2000 Sq Ft Residential Apartment Is Up For Ren...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2,000 sqft</td>\n",
              "      <td>Sector 9, Uttara, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/14...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>October 28, 2021</td>\n",
              "      <td>35 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Strongly Structured This 1650 Sq. ft Apartment...</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,650 sqft</td>\n",
              "      <td>Block I, Bashundhara R-A, Dhaka</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>For Rent</td>\n",
              "      <td>https://images-cdn.bproperty.com/thumbnails/10...</td>\n",
              "      <td>https://www.bproperty.com/en/property/details-...</td>\n",
              "      <td>February 19, 2023</td>\n",
              "      <td>25 Thousand</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a87d2ad-a263-46c3-8ea5-dbe5d32a37b3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5a87d2ad-a263-46c3-8ea5-dbe5d32a37b3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5a87d2ad-a263-46c3-8ea5-dbe5d32a37b3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "non_numeric_cols = ['title', 'adress', 'type', 'purpose', 'flooPlan', 'url', 'lastUpdated']\n",
        "data = data.drop(columns=non_numeric_cols)\n",
        "data.head(6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "IXs2a9rmpjOz",
        "outputId": "9692d24b-df99-4e7d-f2d6-f64077bb9790"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  beds bath        area        price\n",
              "0   3    4   2,200 sqft  50 Thousand\n",
              "1   3    4   1,400 sqft  30 Thousand\n",
              "2   3    4   1,950 sqft  30 Thousand\n",
              "3   3    3   2,000 sqft  35 Thousand\n",
              "4   3    4   1,650 sqft  25 Thousand\n",
              "5   5    5   3,400 sqft     1.1 Lakh"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c55e4544-f178-4897-a27f-f4388f814fdb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>area</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2,200 sqft</td>\n",
              "      <td>50 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,400 sqft</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,950 sqft</td>\n",
              "      <td>30 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2,000 sqft</td>\n",
              "      <td>35 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1,650 sqft</td>\n",
              "      <td>25 Thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3,400 sqft</td>\n",
              "      <td>1.1 Lakh</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c55e4544-f178-4897-a27f-f4388f814fdb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c55e4544-f178-4897-a27f-f4388f814fdb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c55e4544-f178-4897-a27f-f4388f814fdb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['area'] = data['area'].str.replace(',', '').str.replace(' sqft', '').astype(float)\n",
        "data['price'] = data['price'].apply(lambda x: float(x.split()[0]) * 1000 if isinstance(x, str) else x)"
      ],
      "metadata": {
        "id": "zZjLkGOM4wsv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['bath'] = data['bath'].apply(lambda x: float(x.split()[0]) if isinstance(x, str) else x)"
      ],
      "metadata": {
        "id": "qUCc7gJF7HCM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "eKLOXP2X7isq",
        "outputId": "88a551b0-b6de-48dc-dd22-4ce765f406dd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  beds  bath    area    price\n",
              "0   3    4.0  2200.0  50000.0\n",
              "1   3    4.0  1400.0  30000.0\n",
              "2   3    4.0  1950.0  30000.0\n",
              "3   3    3.0  2000.0  35000.0\n",
              "4   3    4.0  1650.0  25000.0\n",
              "5   5    5.0  3400.0   1100.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b283c1bc-9fb8-46e7-8866-f37ca0666c76\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>area</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2200.0</td>\n",
              "      <td>50000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1400.0</td>\n",
              "      <td>30000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1950.0</td>\n",
              "      <td>30000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>35000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1650.0</td>\n",
              "      <td>25000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3400.0</td>\n",
              "      <td>1100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b283c1bc-9fb8-46e7-8866-f37ca0666c76')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b283c1bc-9fb8-46e7-8866-f37ca0666c76 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b283c1bc-9fb8-46e7-8866-f37ca0666c76');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the columns to be normalized\n",
        "numeric_columns = ['area', 'price', 'bath']\n",
        "\n",
        "# Standardization (Z-score)\n",
        "data[numeric_columns] = data[numeric_columns].apply(lambda x: (x - x.mean()) / x.std())\n",
        "\n"
      ],
      "metadata": {
        "id": "JIF_uh4x7rwX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "0LwrUcUSsMmU",
        "outputId": "1793fe2d-55be-4d48-8ae0-e0eebe565e72"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  beds      bath      area     price\n",
              "0   3   1.614978  1.348296  1.837496\n",
              "1   3   1.614978  0.115557  0.523469\n",
              "2   3   1.614978  0.963065  0.523469\n",
              "3   3   0.405884  1.040111  0.851976\n",
              "4   3   1.614978  0.500788  0.194962\n",
              "5   5   2.824072  3.197404 -1.375300"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-794132a9-92b8-4d4d-a5e0-7fa61907828a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>beds</th>\n",
              "      <th>bath</th>\n",
              "      <th>area</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>1.614978</td>\n",
              "      <td>1.348296</td>\n",
              "      <td>1.837496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>1.614978</td>\n",
              "      <td>0.115557</td>\n",
              "      <td>0.523469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.614978</td>\n",
              "      <td>0.963065</td>\n",
              "      <td>0.523469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.405884</td>\n",
              "      <td>1.040111</td>\n",
              "      <td>0.851976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>1.614978</td>\n",
              "      <td>0.500788</td>\n",
              "      <td>0.194962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>2.824072</td>\n",
              "      <td>3.197404</td>\n",
              "      <td>-1.375300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-794132a9-92b8-4d4d-a5e0-7fa61907828a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-794132a9-92b8-4d4d-a5e0-7fa61907828a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-794132a9-92b8-4d4d-a5e0-7fa61907828a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "beds = data['beds']\n",
        "\n",
        "# Extract the numeric part using regular expressions\n",
        "numeric_beds = beds.str.extract(r'(\\d+)').astype(float)\n",
        "\n",
        "# Replace the original column with the extracted numeric values\n",
        "data['beds'] = numeric_beds"
      ],
      "metadata": {
        "id": "MFXNSXnWidRC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(data):\n",
        "  data = data.sample(frac=1).reset_index(drop=True)\n",
        "  train_size = int(len(data)* 0.6)\n",
        "  val_size = int(len(data)* 0.2)\n",
        "\n",
        "  train_data = data[:train_size]\n",
        "  val_data = data[train_size: train_size+val_size]\n",
        "  test_data = data[train_size+val_size:]\n",
        "\n",
        "  y_train_data = train_data['price']\n",
        "  y_val_data = val_data['price']\n",
        "  y_test_data = test_data['price']\n",
        "\n",
        "  X_train_data = train_data.drop(columns=['price'])\n",
        "  X_val_data = val_data.drop(columns=['price'])\n",
        "  X_test_data = test_data.drop(columns=['price'])\n",
        "  return train_data, val_data, test_data,X_train_data,X_val_data,X_test_data,y_train_data,y_val_data,y_test_data\n",
        "\n",
        "train_data, val_data, test_data,X_train_data,X_val_data,X_test_data,y_train_data,y_val_data,y_test_data = split_data(data)"
      ],
      "metadata": {
        "id": "Xo3-hXBdR8F_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lm = LinearRegression()\n",
        "lm.fit(X_train_data, y_train_data)\n",
        "\n",
        "\n",
        "y_pred_from_sklearn = lm.predict(X_test_data)\n",
        "\n",
        "sklearn_pred_df = pd.DataFrame(\n",
        "    {\n",
        "        'Actual Value' : y_test_data,\n",
        "     'Predicted Values' : y_pred_from_sklearn\n",
        "    }\n",
        ")\n",
        "sklearn_pred_df.head(6)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "gWjNYSMYdsSa",
        "outputId": "34434392-908e-493b-f1e0-48aea172da82"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Actual Value  Predicted Values\n",
              "5990      2.494509          0.199913\n",
              "5991     -0.593454         -0.395652\n",
              "5992     -0.527753         -0.388193\n",
              "5993     -0.002142         -0.386328\n",
              "5994     -0.790558         -0.393787\n",
              "5995     -0.790558         -0.838303"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cbff219b-ca16-4e2a-9e14-9ca9a3ee5648\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Actual Value</th>\n",
              "      <th>Predicted Values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5990</th>\n",
              "      <td>2.494509</td>\n",
              "      <td>0.199913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5991</th>\n",
              "      <td>-0.593454</td>\n",
              "      <td>-0.395652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5992</th>\n",
              "      <td>-0.527753</td>\n",
              "      <td>-0.388193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5993</th>\n",
              "      <td>-0.002142</td>\n",
              "      <td>-0.386328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5994</th>\n",
              "      <td>-0.790558</td>\n",
              "      <td>-0.393787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5995</th>\n",
              "      <td>-0.790558</td>\n",
              "      <td>-0.838303</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cbff219b-ca16-4e2a-9e14-9ca9a3ee5648')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cbff219b-ca16-4e2a-9e14-9ca9a3ee5648 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cbff219b-ca16-4e2a-9e14-9ca9a3ee5648');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_standardized = (X_train_data - X_train_data .mean()) / X_train_data.std()\n",
        "X_test_standardized  = (X_test_data - X_train_data.mean()) /  X_train_data.std()\n",
        "X_val_standardized = (X_val_data - X_val_data .mean()) / X_val_data.std()"
      ],
      "metadata": {
        "id": "RYmdlGV8epRQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def error_function(y_actual,y_predicted):\n",
        "    error = 0\n",
        "    for i in range(0,len(y_actual)):\n",
        "        error =  error + pow((y_actual[i] - y_predicted[i]),2)\n",
        "        return error/(2*len(y_actual))"
      ],
      "metadata": {
        "id": "OiHKGZ-Ze1-z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def y_predicted(w,x):\n",
        "    y_pred = np.zeros(len(x))\n",
        "    for i in range(0,len(x)):\n",
        "        for j in range(0,len(w)):\n",
        "            y_pred[i] = y_pred[i]+(w[j]*x[i][j] + w[-1])\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "FSP1PRrEe44p"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(y_actual,y_pred,x):\n",
        "    grad = np.zeros(x.shape[1])\n",
        "    for i in range(x.shape[1]):\n",
        "        for j in range(0,len(y_actual)):\n",
        "            grad[i] = - (y_actual[j] - y_pred[j])*x[j][i] + grad[i]\n",
        "    return grad/len(y_actual)\n"
      ],
      "metadata": {
        "id": "nm8zMtiMe74y"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def weights(x_train,y_train,x_val, y_val,num_iterations,learning_rate):\n",
        "    no_of_rows = x_train.shape[0]\n",
        "    no_of_columns = x_train.shape[1]\n",
        "    new_x_train = np.ones((no_of_rows,no_of_columns+1))\n",
        "    new_x_train[:,0:no_of_columns] = x_train\n",
        "    w = np.zeros(no_of_columns)\n",
        "    w =np.append(w,1)\n",
        "    for i in range(0,num_iterations):\n",
        "        y_pred_train = y_predicted(w,new_x_train)\n",
        "        error_train = error_function(y_train,y_pred_train)\n",
        "\n",
        "        new_x_val = np.ones((x_val.shape[0], x_val.shape[1] + 1))\n",
        "        new_x_val[:, 0:x_val.shape[1]] = x_val\n",
        "        y_pred_val = y_predicted(w, new_x_val)\n",
        "        error_val = error_function(y_val, y_pred_val)\n",
        "\n",
        "        print(\"Training Loss: \",error_train,\" Validation Loss: \", error_val)\n",
        "\n",
        "        MSE_points.append(error_train)\n",
        "        grad = gradient_descent(y_train, y_pred_train, new_x_train)\n",
        "        w = w - learning_rate * grad\n",
        "        learning_rate = learning_rate / 1.05\n",
        "    return w\n",
        "    '''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "ciCaazZFe_nT",
        "outputId": "cefa4f05-6be4-4475-d4e4-263eeb1fcb85"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef weights(x_train,y_train,x_val, y_val,num_iterations,learning_rate):\\n    no_of_rows = x_train.shape[0]\\n    no_of_columns = x_train.shape[1]\\n    new_x_train = np.ones((no_of_rows,no_of_columns+1))\\n    new_x_train[:,0:no_of_columns] = x_train\\n    w = np.zeros(no_of_columns)\\n    w =np.append(w,1)\\n    for i in range(0,num_iterations):\\n        y_pred_train = y_predicted(w,new_x_train)\\n        error_train = error_function(y_train,y_pred_train)\\n\\n        new_x_val = np.ones((x_val.shape[0], x_val.shape[1] + 1))\\n        new_x_val[:, 0:x_val.shape[1]] = x_val\\n        y_pred_val = y_predicted(w, new_x_val)\\n        error_val = error_function(y_val, y_pred_val)\\n\\n        print(\"Training Loss: \",error_train,\" Validation Loss: \", error_val)\\n\\n        MSE_points.append(error_train)\\n        grad = gradient_descent(y_train, y_pred_train, new_x_train)\\n        w = w - learning_rate * grad\\n        learning_rate = learning_rate / 1.05\\n    return w\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def regression_test(x_test,w):\n",
        "    row = x_test.shape[0]\n",
        "    column = x_test.shape[1]\n",
        "    new_x_test = np.ones((row,column+1))\n",
        "    new_x_test[:,0:column] = x_test\n",
        "    y_pred = y_predicted(w,new_x_test)\n",
        "    return(y_pred)"
      ],
      "metadata": {
        "id": "0Ekrq0tXfCZp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE_train_points = []\n",
        "MSE_val_points = []\n",
        "num_iterations = 1000\n",
        "learning_rate = 0.01\n",
        "w = np.zeros(X_train_standardized.shape[1] + 1)\n",
        "\n",
        "# Calculate predictions on training and validation data\n",
        "y_train_pred = regression_test(X_train_standardized, w)\n",
        "y_val_pred = regression_test(X_val_standardized, w)\n",
        "\n",
        "# Calculate training and validation loss\n",
        "train_loss = error_function(y_train_data.values, y_train_pred)\n",
        "val_loss = error_function(y_val_data.values, y_val_pred)\n",
        "\n",
        "# Print training and validation loss per epoch\n",
        "for i in range(num_iterations):\n",
        "    print(f\"Iteration {i+1:4d}: Training Loss {train_loss:.8f}, Validation Loss {val_loss:.8f}\")\n",
        "    MSE_train_points.append(train_loss)\n",
        "    MSE_val_points.append(val_loss)\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    grad = gradient_descent(y_train_data.values, y_train_pred, X_train_standardized.values)\n",
        "    w[:-1] = w[:-1] - learning_rate * grad\n",
        "\n",
        "    # Recalculate predictions on training and validation data\n",
        "    y_train_pred = regression_test(X_train_standardized, w)\n",
        "    y_val_pred = regression_test(X_val_standardized, w)\n",
        "\n",
        "    # Recalculate training and validation loss\n",
        "    train_loss = error_function(y_train_data.values, y_train_pred)\n",
        "    val_loss = error_function(y_val_data.values, y_val_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-69i9M1rZ7u",
        "outputId": "bf079f9d-709a-4df3-ae38-f88bd241f9bf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration    1: Training Loss 0.00000781, Validation Loss 0.00014512\n",
            "Iteration    2: Training Loss 0.00000719, Validation Loss 0.00014047\n",
            "Iteration    3: Training Loss 0.00000661, Validation Loss 0.00013601\n",
            "Iteration    4: Training Loss 0.00000606, Validation Loss 0.00013174\n",
            "Iteration    5: Training Loss 0.00000556, Validation Loss 0.00012764\n",
            "Iteration    6: Training Loss 0.00000508, Validation Loss 0.00012370\n",
            "Iteration    7: Training Loss 0.00000464, Validation Loss 0.00011993\n",
            "Iteration    8: Training Loss 0.00000423, Validation Loss 0.00011630\n",
            "Iteration    9: Training Loss 0.00000385, Validation Loss 0.00011282\n",
            "Iteration   10: Training Loss 0.00000349, Validation Loss 0.00010948\n",
            "Iteration   11: Training Loss 0.00000316, Validation Loss 0.00010628\n",
            "Iteration   12: Training Loss 0.00000286, Validation Loss 0.00010320\n",
            "Iteration   13: Training Loss 0.00000258, Validation Loss 0.00010024\n",
            "Iteration   14: Training Loss 0.00000231, Validation Loss 0.00009740\n",
            "Iteration   15: Training Loss 0.00000207, Validation Loss 0.00009467\n",
            "Iteration   16: Training Loss 0.00000185, Validation Loss 0.00009204\n",
            "Iteration   17: Training Loss 0.00000165, Validation Loss 0.00008952\n",
            "Iteration   18: Training Loss 0.00000146, Validation Loss 0.00008710\n",
            "Iteration   19: Training Loss 0.00000129, Validation Loss 0.00008477\n",
            "Iteration   20: Training Loss 0.00000113, Validation Loss 0.00008253\n",
            "Iteration   21: Training Loss 0.00000098, Validation Loss 0.00008037\n",
            "Iteration   22: Training Loss 0.00000085, Validation Loss 0.00007830\n",
            "Iteration   23: Training Loss 0.00000074, Validation Loss 0.00007630\n",
            "Iteration   24: Training Loss 0.00000063, Validation Loss 0.00007438\n",
            "Iteration   25: Training Loss 0.00000053, Validation Loss 0.00007254\n",
            "Iteration   26: Training Loss 0.00000045, Validation Loss 0.00007076\n",
            "Iteration   27: Training Loss 0.00000037, Validation Loss 0.00006905\n",
            "Iteration   28: Training Loss 0.00000030, Validation Loss 0.00006740\n",
            "Iteration   29: Training Loss 0.00000024, Validation Loss 0.00006582\n",
            "Iteration   30: Training Loss 0.00000019, Validation Loss 0.00006429\n",
            "Iteration   31: Training Loss 0.00000015, Validation Loss 0.00006282\n",
            "Iteration   32: Training Loss 0.00000011, Validation Loss 0.00006140\n",
            "Iteration   33: Training Loss 0.00000008, Validation Loss 0.00006003\n",
            "Iteration   34: Training Loss 0.00000005, Validation Loss 0.00005871\n",
            "Iteration   35: Training Loss 0.00000003, Validation Loss 0.00005745\n",
            "Iteration   36: Training Loss 0.00000002, Validation Loss 0.00005622\n",
            "Iteration   37: Training Loss 0.00000001, Validation Loss 0.00005504\n",
            "Iteration   38: Training Loss 0.00000000, Validation Loss 0.00005391\n",
            "Iteration   39: Training Loss 0.00000000, Validation Loss 0.00005281\n",
            "Iteration   40: Training Loss 0.00000000, Validation Loss 0.00005175\n",
            "Iteration   41: Training Loss 0.00000001, Validation Loss 0.00005073\n",
            "Iteration   42: Training Loss 0.00000002, Validation Loss 0.00004974\n",
            "Iteration   43: Training Loss 0.00000003, Validation Loss 0.00004879\n",
            "Iteration   44: Training Loss 0.00000004, Validation Loss 0.00004788\n",
            "Iteration   45: Training Loss 0.00000006, Validation Loss 0.00004699\n",
            "Iteration   46: Training Loss 0.00000008, Validation Loss 0.00004613\n",
            "Iteration   47: Training Loss 0.00000010, Validation Loss 0.00004531\n",
            "Iteration   48: Training Loss 0.00000012, Validation Loss 0.00004451\n",
            "Iteration   49: Training Loss 0.00000015, Validation Loss 0.00004374\n",
            "Iteration   50: Training Loss 0.00000017, Validation Loss 0.00004300\n",
            "Iteration   51: Training Loss 0.00000020, Validation Loss 0.00004228\n",
            "Iteration   52: Training Loss 0.00000023, Validation Loss 0.00004158\n",
            "Iteration   53: Training Loss 0.00000026, Validation Loss 0.00004091\n",
            "Iteration   54: Training Loss 0.00000029, Validation Loss 0.00004026\n",
            "Iteration   55: Training Loss 0.00000032, Validation Loss 0.00003963\n",
            "Iteration   56: Training Loss 0.00000036, Validation Loss 0.00003903\n",
            "Iteration   57: Training Loss 0.00000039, Validation Loss 0.00003844\n",
            "Iteration   58: Training Loss 0.00000042, Validation Loss 0.00003787\n",
            "Iteration   59: Training Loss 0.00000046, Validation Loss 0.00003733\n",
            "Iteration   60: Training Loss 0.00000049, Validation Loss 0.00003680\n",
            "Iteration   61: Training Loss 0.00000053, Validation Loss 0.00003628\n",
            "Iteration   62: Training Loss 0.00000056, Validation Loss 0.00003579\n",
            "Iteration   63: Training Loss 0.00000060, Validation Loss 0.00003531\n",
            "Iteration   64: Training Loss 0.00000064, Validation Loss 0.00003484\n",
            "Iteration   65: Training Loss 0.00000067, Validation Loss 0.00003439\n",
            "Iteration   66: Training Loss 0.00000071, Validation Loss 0.00003395\n",
            "Iteration   67: Training Loss 0.00000075, Validation Loss 0.00003353\n",
            "Iteration   68: Training Loss 0.00000078, Validation Loss 0.00003312\n",
            "Iteration   69: Training Loss 0.00000082, Validation Loss 0.00003273\n",
            "Iteration   70: Training Loss 0.00000085, Validation Loss 0.00003235\n",
            "Iteration   71: Training Loss 0.00000089, Validation Loss 0.00003197\n",
            "Iteration   72: Training Loss 0.00000092, Validation Loss 0.00003162\n",
            "Iteration   73: Training Loss 0.00000096, Validation Loss 0.00003127\n",
            "Iteration   74: Training Loss 0.00000099, Validation Loss 0.00003093\n",
            "Iteration   75: Training Loss 0.00000103, Validation Loss 0.00003060\n",
            "Iteration   76: Training Loss 0.00000106, Validation Loss 0.00003029\n",
            "Iteration   77: Training Loss 0.00000110, Validation Loss 0.00002998\n",
            "Iteration   78: Training Loss 0.00000113, Validation Loss 0.00002968\n",
            "Iteration   79: Training Loss 0.00000116, Validation Loss 0.00002939\n",
            "Iteration   80: Training Loss 0.00000120, Validation Loss 0.00002912\n",
            "Iteration   81: Training Loss 0.00000123, Validation Loss 0.00002884\n",
            "Iteration   82: Training Loss 0.00000126, Validation Loss 0.00002858\n",
            "Iteration   83: Training Loss 0.00000129, Validation Loss 0.00002833\n",
            "Iteration   84: Training Loss 0.00000132, Validation Loss 0.00002808\n",
            "Iteration   85: Training Loss 0.00000135, Validation Loss 0.00002784\n",
            "Iteration   86: Training Loss 0.00000138, Validation Loss 0.00002761\n",
            "Iteration   87: Training Loss 0.00000141, Validation Loss 0.00002738\n",
            "Iteration   88: Training Loss 0.00000144, Validation Loss 0.00002717\n",
            "Iteration   89: Training Loss 0.00000147, Validation Loss 0.00002695\n",
            "Iteration   90: Training Loss 0.00000150, Validation Loss 0.00002675\n",
            "Iteration   91: Training Loss 0.00000153, Validation Loss 0.00002655\n",
            "Iteration   92: Training Loss 0.00000156, Validation Loss 0.00002636\n",
            "Iteration   93: Training Loss 0.00000158, Validation Loss 0.00002617\n",
            "Iteration   94: Training Loss 0.00000161, Validation Loss 0.00002599\n",
            "Iteration   95: Training Loss 0.00000163, Validation Loss 0.00002581\n",
            "Iteration   96: Training Loss 0.00000166, Validation Loss 0.00002564\n",
            "Iteration   97: Training Loss 0.00000168, Validation Loss 0.00002547\n",
            "Iteration   98: Training Loss 0.00000171, Validation Loss 0.00002531\n",
            "Iteration   99: Training Loss 0.00000173, Validation Loss 0.00002516\n",
            "Iteration  100: Training Loss 0.00000176, Validation Loss 0.00002500\n",
            "Iteration  101: Training Loss 0.00000178, Validation Loss 0.00002486\n",
            "Iteration  102: Training Loss 0.00000180, Validation Loss 0.00002471\n",
            "Iteration  103: Training Loss 0.00000182, Validation Loss 0.00002457\n",
            "Iteration  104: Training Loss 0.00000184, Validation Loss 0.00002444\n",
            "Iteration  105: Training Loss 0.00000187, Validation Loss 0.00002431\n",
            "Iteration  106: Training Loss 0.00000189, Validation Loss 0.00002418\n",
            "Iteration  107: Training Loss 0.00000191, Validation Loss 0.00002406\n",
            "Iteration  108: Training Loss 0.00000193, Validation Loss 0.00002394\n",
            "Iteration  109: Training Loss 0.00000195, Validation Loss 0.00002382\n",
            "Iteration  110: Training Loss 0.00000196, Validation Loss 0.00002371\n",
            "Iteration  111: Training Loss 0.00000198, Validation Loss 0.00002360\n",
            "Iteration  112: Training Loss 0.00000200, Validation Loss 0.00002349\n",
            "Iteration  113: Training Loss 0.00000202, Validation Loss 0.00002339\n",
            "Iteration  114: Training Loss 0.00000204, Validation Loss 0.00002329\n",
            "Iteration  115: Training Loss 0.00000205, Validation Loss 0.00002319\n",
            "Iteration  116: Training Loss 0.00000207, Validation Loss 0.00002310\n",
            "Iteration  117: Training Loss 0.00000208, Validation Loss 0.00002301\n",
            "Iteration  118: Training Loss 0.00000210, Validation Loss 0.00002292\n",
            "Iteration  119: Training Loss 0.00000212, Validation Loss 0.00002283\n",
            "Iteration  120: Training Loss 0.00000213, Validation Loss 0.00002275\n",
            "Iteration  121: Training Loss 0.00000214, Validation Loss 0.00002267\n",
            "Iteration  122: Training Loss 0.00000216, Validation Loss 0.00002259\n",
            "Iteration  123: Training Loss 0.00000217, Validation Loss 0.00002251\n",
            "Iteration  124: Training Loss 0.00000219, Validation Loss 0.00002244\n",
            "Iteration  125: Training Loss 0.00000220, Validation Loss 0.00002236\n",
            "Iteration  126: Training Loss 0.00000221, Validation Loss 0.00002229\n",
            "Iteration  127: Training Loss 0.00000222, Validation Loss 0.00002222\n",
            "Iteration  128: Training Loss 0.00000224, Validation Loss 0.00002216\n",
            "Iteration  129: Training Loss 0.00000225, Validation Loss 0.00002209\n",
            "Iteration  130: Training Loss 0.00000226, Validation Loss 0.00002203\n",
            "Iteration  131: Training Loss 0.00000227, Validation Loss 0.00002197\n",
            "Iteration  132: Training Loss 0.00000228, Validation Loss 0.00002191\n",
            "Iteration  133: Training Loss 0.00000229, Validation Loss 0.00002186\n",
            "Iteration  134: Training Loss 0.00000230, Validation Loss 0.00002180\n",
            "Iteration  135: Training Loss 0.00000231, Validation Loss 0.00002175\n",
            "Iteration  136: Training Loss 0.00000232, Validation Loss 0.00002169\n",
            "Iteration  137: Training Loss 0.00000233, Validation Loss 0.00002164\n",
            "Iteration  138: Training Loss 0.00000234, Validation Loss 0.00002160\n",
            "Iteration  139: Training Loss 0.00000235, Validation Loss 0.00002155\n",
            "Iteration  140: Training Loss 0.00000236, Validation Loss 0.00002150\n",
            "Iteration  141: Training Loss 0.00000237, Validation Loss 0.00002146\n",
            "Iteration  142: Training Loss 0.00000238, Validation Loss 0.00002141\n",
            "Iteration  143: Training Loss 0.00000239, Validation Loss 0.00002137\n",
            "Iteration  144: Training Loss 0.00000239, Validation Loss 0.00002133\n",
            "Iteration  145: Training Loss 0.00000240, Validation Loss 0.00002129\n",
            "Iteration  146: Training Loss 0.00000241, Validation Loss 0.00002125\n",
            "Iteration  147: Training Loss 0.00000242, Validation Loss 0.00002122\n",
            "Iteration  148: Training Loss 0.00000242, Validation Loss 0.00002118\n",
            "Iteration  149: Training Loss 0.00000243, Validation Loss 0.00002114\n",
            "Iteration  150: Training Loss 0.00000244, Validation Loss 0.00002111\n",
            "Iteration  151: Training Loss 0.00000244, Validation Loss 0.00002108\n",
            "Iteration  152: Training Loss 0.00000245, Validation Loss 0.00002105\n",
            "Iteration  153: Training Loss 0.00000246, Validation Loss 0.00002102\n",
            "Iteration  154: Training Loss 0.00000246, Validation Loss 0.00002099\n",
            "Iteration  155: Training Loss 0.00000247, Validation Loss 0.00002096\n",
            "Iteration  156: Training Loss 0.00000247, Validation Loss 0.00002093\n",
            "Iteration  157: Training Loss 0.00000248, Validation Loss 0.00002090\n",
            "Iteration  158: Training Loss 0.00000248, Validation Loss 0.00002088\n",
            "Iteration  159: Training Loss 0.00000249, Validation Loss 0.00002085\n",
            "Iteration  160: Training Loss 0.00000249, Validation Loss 0.00002083\n",
            "Iteration  161: Training Loss 0.00000250, Validation Loss 0.00002080\n",
            "Iteration  162: Training Loss 0.00000250, Validation Loss 0.00002078\n",
            "Iteration  163: Training Loss 0.00000251, Validation Loss 0.00002076\n",
            "Iteration  164: Training Loss 0.00000251, Validation Loss 0.00002074\n",
            "Iteration  165: Training Loss 0.00000251, Validation Loss 0.00002072\n",
            "Iteration  166: Training Loss 0.00000252, Validation Loss 0.00002069\n",
            "Iteration  167: Training Loss 0.00000252, Validation Loss 0.00002068\n",
            "Iteration  168: Training Loss 0.00000253, Validation Loss 0.00002066\n",
            "Iteration  169: Training Loss 0.00000253, Validation Loss 0.00002064\n",
            "Iteration  170: Training Loss 0.00000253, Validation Loss 0.00002062\n",
            "Iteration  171: Training Loss 0.00000254, Validation Loss 0.00002060\n",
            "Iteration  172: Training Loss 0.00000254, Validation Loss 0.00002059\n",
            "Iteration  173: Training Loss 0.00000254, Validation Loss 0.00002057\n",
            "Iteration  174: Training Loss 0.00000255, Validation Loss 0.00002056\n",
            "Iteration  175: Training Loss 0.00000255, Validation Loss 0.00002054\n",
            "Iteration  176: Training Loss 0.00000255, Validation Loss 0.00002053\n",
            "Iteration  177: Training Loss 0.00000255, Validation Loss 0.00002052\n",
            "Iteration  178: Training Loss 0.00000256, Validation Loss 0.00002050\n",
            "Iteration  179: Training Loss 0.00000256, Validation Loss 0.00002049\n",
            "Iteration  180: Training Loss 0.00000256, Validation Loss 0.00002048\n",
            "Iteration  181: Training Loss 0.00000256, Validation Loss 0.00002047\n",
            "Iteration  182: Training Loss 0.00000257, Validation Loss 0.00002046\n",
            "Iteration  183: Training Loss 0.00000257, Validation Loss 0.00002044\n",
            "Iteration  184: Training Loss 0.00000257, Validation Loss 0.00002043\n",
            "Iteration  185: Training Loss 0.00000257, Validation Loss 0.00002042\n",
            "Iteration  186: Training Loss 0.00000257, Validation Loss 0.00002042\n",
            "Iteration  187: Training Loss 0.00000258, Validation Loss 0.00002041\n",
            "Iteration  188: Training Loss 0.00000258, Validation Loss 0.00002040\n",
            "Iteration  189: Training Loss 0.00000258, Validation Loss 0.00002039\n",
            "Iteration  190: Training Loss 0.00000258, Validation Loss 0.00002038\n",
            "Iteration  191: Training Loss 0.00000258, Validation Loss 0.00002037\n",
            "Iteration  192: Training Loss 0.00000258, Validation Loss 0.00002037\n",
            "Iteration  193: Training Loss 0.00000259, Validation Loss 0.00002036\n",
            "Iteration  194: Training Loss 0.00000259, Validation Loss 0.00002035\n",
            "Iteration  195: Training Loss 0.00000259, Validation Loss 0.00002035\n",
            "Iteration  196: Training Loss 0.00000259, Validation Loss 0.00002034\n",
            "Iteration  197: Training Loss 0.00000259, Validation Loss 0.00002034\n",
            "Iteration  198: Training Loss 0.00000259, Validation Loss 0.00002033\n",
            "Iteration  199: Training Loss 0.00000259, Validation Loss 0.00002033\n",
            "Iteration  200: Training Loss 0.00000259, Validation Loss 0.00002032\n",
            "Iteration  201: Training Loss 0.00000259, Validation Loss 0.00002032\n",
            "Iteration  202: Training Loss 0.00000260, Validation Loss 0.00002031\n",
            "Iteration  203: Training Loss 0.00000260, Validation Loss 0.00002031\n",
            "Iteration  204: Training Loss 0.00000260, Validation Loss 0.00002030\n",
            "Iteration  205: Training Loss 0.00000260, Validation Loss 0.00002030\n",
            "Iteration  206: Training Loss 0.00000260, Validation Loss 0.00002030\n",
            "Iteration  207: Training Loss 0.00000260, Validation Loss 0.00002030\n",
            "Iteration  208: Training Loss 0.00000260, Validation Loss 0.00002029\n",
            "Iteration  209: Training Loss 0.00000260, Validation Loss 0.00002029\n",
            "Iteration  210: Training Loss 0.00000260, Validation Loss 0.00002029\n",
            "Iteration  211: Training Loss 0.00000260, Validation Loss 0.00002029\n",
            "Iteration  212: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  213: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  214: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  215: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  216: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  217: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  218: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  219: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  220: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  221: Training Loss 0.00000260, Validation Loss 0.00002027\n",
            "Iteration  222: Training Loss 0.00000260, Validation Loss 0.00002027\n",
            "Iteration  223: Training Loss 0.00000260, Validation Loss 0.00002027\n",
            "Iteration  224: Training Loss 0.00000260, Validation Loss 0.00002027\n",
            "Iteration  225: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  226: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  227: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  228: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  229: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  230: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  231: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  232: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  233: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  234: Training Loss 0.00000260, Validation Loss 0.00002028\n",
            "Iteration  235: Training Loss 0.00000260, Validation Loss 0.00002029\n",
            "Iteration  236: Training Loss 0.00000260, Validation Loss 0.00002029\n",
            "Iteration  237: Training Loss 0.00000260, Validation Loss 0.00002029\n",
            "Iteration  238: Training Loss 0.00000260, Validation Loss 0.00002029\n",
            "Iteration  239: Training Loss 0.00000260, Validation Loss 0.00002029\n",
            "Iteration  240: Training Loss 0.00000260, Validation Loss 0.00002029\n",
            "Iteration  241: Training Loss 0.00000260, Validation Loss 0.00002030\n",
            "Iteration  242: Training Loss 0.00000259, Validation Loss 0.00002030\n",
            "Iteration  243: Training Loss 0.00000259, Validation Loss 0.00002030\n",
            "Iteration  244: Training Loss 0.00000259, Validation Loss 0.00002030\n",
            "Iteration  245: Training Loss 0.00000259, Validation Loss 0.00002031\n",
            "Iteration  246: Training Loss 0.00000259, Validation Loss 0.00002031\n",
            "Iteration  247: Training Loss 0.00000259, Validation Loss 0.00002031\n",
            "Iteration  248: Training Loss 0.00000259, Validation Loss 0.00002031\n",
            "Iteration  249: Training Loss 0.00000259, Validation Loss 0.00002032\n",
            "Iteration  250: Training Loss 0.00000259, Validation Loss 0.00002032\n",
            "Iteration  251: Training Loss 0.00000259, Validation Loss 0.00002032\n",
            "Iteration  252: Training Loss 0.00000259, Validation Loss 0.00002032\n",
            "Iteration  253: Training Loss 0.00000259, Validation Loss 0.00002033\n",
            "Iteration  254: Training Loss 0.00000259, Validation Loss 0.00002033\n",
            "Iteration  255: Training Loss 0.00000259, Validation Loss 0.00002033\n",
            "Iteration  256: Training Loss 0.00000259, Validation Loss 0.00002034\n",
            "Iteration  257: Training Loss 0.00000259, Validation Loss 0.00002034\n",
            "Iteration  258: Training Loss 0.00000258, Validation Loss 0.00002034\n",
            "Iteration  259: Training Loss 0.00000258, Validation Loss 0.00002035\n",
            "Iteration  260: Training Loss 0.00000258, Validation Loss 0.00002035\n",
            "Iteration  261: Training Loss 0.00000258, Validation Loss 0.00002035\n",
            "Iteration  262: Training Loss 0.00000258, Validation Loss 0.00002036\n",
            "Iteration  263: Training Loss 0.00000258, Validation Loss 0.00002036\n",
            "Iteration  264: Training Loss 0.00000258, Validation Loss 0.00002036\n",
            "Iteration  265: Training Loss 0.00000258, Validation Loss 0.00002037\n",
            "Iteration  266: Training Loss 0.00000258, Validation Loss 0.00002037\n",
            "Iteration  267: Training Loss 0.00000258, Validation Loss 0.00002038\n",
            "Iteration  268: Training Loss 0.00000258, Validation Loss 0.00002038\n",
            "Iteration  269: Training Loss 0.00000258, Validation Loss 0.00002038\n",
            "Iteration  270: Training Loss 0.00000257, Validation Loss 0.00002039\n",
            "Iteration  271: Training Loss 0.00000257, Validation Loss 0.00002039\n",
            "Iteration  272: Training Loss 0.00000257, Validation Loss 0.00002040\n",
            "Iteration  273: Training Loss 0.00000257, Validation Loss 0.00002040\n",
            "Iteration  274: Training Loss 0.00000257, Validation Loss 0.00002040\n",
            "Iteration  275: Training Loss 0.00000257, Validation Loss 0.00002041\n",
            "Iteration  276: Training Loss 0.00000257, Validation Loss 0.00002041\n",
            "Iteration  277: Training Loss 0.00000257, Validation Loss 0.00002042\n",
            "Iteration  278: Training Loss 0.00000257, Validation Loss 0.00002042\n",
            "Iteration  279: Training Loss 0.00000257, Validation Loss 0.00002042\n",
            "Iteration  280: Training Loss 0.00000257, Validation Loss 0.00002043\n",
            "Iteration  281: Training Loss 0.00000256, Validation Loss 0.00002043\n",
            "Iteration  282: Training Loss 0.00000256, Validation Loss 0.00002044\n",
            "Iteration  283: Training Loss 0.00000256, Validation Loss 0.00002044\n",
            "Iteration  284: Training Loss 0.00000256, Validation Loss 0.00002044\n",
            "Iteration  285: Training Loss 0.00000256, Validation Loss 0.00002045\n",
            "Iteration  286: Training Loss 0.00000256, Validation Loss 0.00002045\n",
            "Iteration  287: Training Loss 0.00000256, Validation Loss 0.00002046\n",
            "Iteration  288: Training Loss 0.00000256, Validation Loss 0.00002046\n",
            "Iteration  289: Training Loss 0.00000256, Validation Loss 0.00002047\n",
            "Iteration  290: Training Loss 0.00000256, Validation Loss 0.00002047\n",
            "Iteration  291: Training Loss 0.00000255, Validation Loss 0.00002048\n",
            "Iteration  292: Training Loss 0.00000255, Validation Loss 0.00002048\n",
            "Iteration  293: Training Loss 0.00000255, Validation Loss 0.00002048\n",
            "Iteration  294: Training Loss 0.00000255, Validation Loss 0.00002049\n",
            "Iteration  295: Training Loss 0.00000255, Validation Loss 0.00002049\n",
            "Iteration  296: Training Loss 0.00000255, Validation Loss 0.00002050\n",
            "Iteration  297: Training Loss 0.00000255, Validation Loss 0.00002050\n",
            "Iteration  298: Training Loss 0.00000255, Validation Loss 0.00002051\n",
            "Iteration  299: Training Loss 0.00000255, Validation Loss 0.00002051\n",
            "Iteration  300: Training Loss 0.00000255, Validation Loss 0.00002052\n",
            "Iteration  301: Training Loss 0.00000254, Validation Loss 0.00002052\n",
            "Iteration  302: Training Loss 0.00000254, Validation Loss 0.00002053\n",
            "Iteration  303: Training Loss 0.00000254, Validation Loss 0.00002053\n",
            "Iteration  304: Training Loss 0.00000254, Validation Loss 0.00002053\n",
            "Iteration  305: Training Loss 0.00000254, Validation Loss 0.00002054\n",
            "Iteration  306: Training Loss 0.00000254, Validation Loss 0.00002054\n",
            "Iteration  307: Training Loss 0.00000254, Validation Loss 0.00002055\n",
            "Iteration  308: Training Loss 0.00000254, Validation Loss 0.00002055\n",
            "Iteration  309: Training Loss 0.00000254, Validation Loss 0.00002056\n",
            "Iteration  310: Training Loss 0.00000254, Validation Loss 0.00002056\n",
            "Iteration  311: Training Loss 0.00000253, Validation Loss 0.00002057\n",
            "Iteration  312: Training Loss 0.00000253, Validation Loss 0.00002057\n",
            "Iteration  313: Training Loss 0.00000253, Validation Loss 0.00002058\n",
            "Iteration  314: Training Loss 0.00000253, Validation Loss 0.00002058\n",
            "Iteration  315: Training Loss 0.00000253, Validation Loss 0.00002059\n",
            "Iteration  316: Training Loss 0.00000253, Validation Loss 0.00002059\n",
            "Iteration  317: Training Loss 0.00000253, Validation Loss 0.00002060\n",
            "Iteration  318: Training Loss 0.00000253, Validation Loss 0.00002060\n",
            "Iteration  319: Training Loss 0.00000253, Validation Loss 0.00002060\n",
            "Iteration  320: Training Loss 0.00000253, Validation Loss 0.00002061\n",
            "Iteration  321: Training Loss 0.00000252, Validation Loss 0.00002061\n",
            "Iteration  322: Training Loss 0.00000252, Validation Loss 0.00002062\n",
            "Iteration  323: Training Loss 0.00000252, Validation Loss 0.00002062\n",
            "Iteration  324: Training Loss 0.00000252, Validation Loss 0.00002063\n",
            "Iteration  325: Training Loss 0.00000252, Validation Loss 0.00002063\n",
            "Iteration  326: Training Loss 0.00000252, Validation Loss 0.00002064\n",
            "Iteration  327: Training Loss 0.00000252, Validation Loss 0.00002064\n",
            "Iteration  328: Training Loss 0.00000252, Validation Loss 0.00002065\n",
            "Iteration  329: Training Loss 0.00000252, Validation Loss 0.00002065\n",
            "Iteration  330: Training Loss 0.00000252, Validation Loss 0.00002066\n",
            "Iteration  331: Training Loss 0.00000251, Validation Loss 0.00002066\n",
            "Iteration  332: Training Loss 0.00000251, Validation Loss 0.00002067\n",
            "Iteration  333: Training Loss 0.00000251, Validation Loss 0.00002067\n",
            "Iteration  334: Training Loss 0.00000251, Validation Loss 0.00002068\n",
            "Iteration  335: Training Loss 0.00000251, Validation Loss 0.00002068\n",
            "Iteration  336: Training Loss 0.00000251, Validation Loss 0.00002069\n",
            "Iteration  337: Training Loss 0.00000251, Validation Loss 0.00002069\n",
            "Iteration  338: Training Loss 0.00000251, Validation Loss 0.00002069\n",
            "Iteration  339: Training Loss 0.00000251, Validation Loss 0.00002070\n",
            "Iteration  340: Training Loss 0.00000250, Validation Loss 0.00002070\n",
            "Iteration  341: Training Loss 0.00000250, Validation Loss 0.00002071\n",
            "Iteration  342: Training Loss 0.00000250, Validation Loss 0.00002071\n",
            "Iteration  343: Training Loss 0.00000250, Validation Loss 0.00002072\n",
            "Iteration  344: Training Loss 0.00000250, Validation Loss 0.00002072\n",
            "Iteration  345: Training Loss 0.00000250, Validation Loss 0.00002073\n",
            "Iteration  346: Training Loss 0.00000250, Validation Loss 0.00002073\n",
            "Iteration  347: Training Loss 0.00000250, Validation Loss 0.00002074\n",
            "Iteration  348: Training Loss 0.00000250, Validation Loss 0.00002074\n",
            "Iteration  349: Training Loss 0.00000250, Validation Loss 0.00002075\n",
            "Iteration  350: Training Loss 0.00000249, Validation Loss 0.00002075\n",
            "Iteration  351: Training Loss 0.00000249, Validation Loss 0.00002076\n",
            "Iteration  352: Training Loss 0.00000249, Validation Loss 0.00002076\n",
            "Iteration  353: Training Loss 0.00000249, Validation Loss 0.00002077\n",
            "Iteration  354: Training Loss 0.00000249, Validation Loss 0.00002077\n",
            "Iteration  355: Training Loss 0.00000249, Validation Loss 0.00002078\n",
            "Iteration  356: Training Loss 0.00000249, Validation Loss 0.00002078\n",
            "Iteration  357: Training Loss 0.00000249, Validation Loss 0.00002078\n",
            "Iteration  358: Training Loss 0.00000249, Validation Loss 0.00002079\n",
            "Iteration  359: Training Loss 0.00000249, Validation Loss 0.00002079\n",
            "Iteration  360: Training Loss 0.00000248, Validation Loss 0.00002080\n",
            "Iteration  361: Training Loss 0.00000248, Validation Loss 0.00002080\n",
            "Iteration  362: Training Loss 0.00000248, Validation Loss 0.00002081\n",
            "Iteration  363: Training Loss 0.00000248, Validation Loss 0.00002081\n",
            "Iteration  364: Training Loss 0.00000248, Validation Loss 0.00002082\n",
            "Iteration  365: Training Loss 0.00000248, Validation Loss 0.00002082\n",
            "Iteration  366: Training Loss 0.00000248, Validation Loss 0.00002083\n",
            "Iteration  367: Training Loss 0.00000248, Validation Loss 0.00002083\n",
            "Iteration  368: Training Loss 0.00000248, Validation Loss 0.00002084\n",
            "Iteration  369: Training Loss 0.00000248, Validation Loss 0.00002084\n",
            "Iteration  370: Training Loss 0.00000247, Validation Loss 0.00002085\n",
            "Iteration  371: Training Loss 0.00000247, Validation Loss 0.00002085\n",
            "Iteration  372: Training Loss 0.00000247, Validation Loss 0.00002085\n",
            "Iteration  373: Training Loss 0.00000247, Validation Loss 0.00002086\n",
            "Iteration  374: Training Loss 0.00000247, Validation Loss 0.00002086\n",
            "Iteration  375: Training Loss 0.00000247, Validation Loss 0.00002087\n",
            "Iteration  376: Training Loss 0.00000247, Validation Loss 0.00002087\n",
            "Iteration  377: Training Loss 0.00000247, Validation Loss 0.00002088\n",
            "Iteration  378: Training Loss 0.00000247, Validation Loss 0.00002088\n",
            "Iteration  379: Training Loss 0.00000247, Validation Loss 0.00002089\n",
            "Iteration  380: Training Loss 0.00000246, Validation Loss 0.00002089\n",
            "Iteration  381: Training Loss 0.00000246, Validation Loss 0.00002090\n",
            "Iteration  382: Training Loss 0.00000246, Validation Loss 0.00002090\n",
            "Iteration  383: Training Loss 0.00000246, Validation Loss 0.00002091\n",
            "Iteration  384: Training Loss 0.00000246, Validation Loss 0.00002091\n",
            "Iteration  385: Training Loss 0.00000246, Validation Loss 0.00002091\n",
            "Iteration  386: Training Loss 0.00000246, Validation Loss 0.00002092\n",
            "Iteration  387: Training Loss 0.00000246, Validation Loss 0.00002092\n",
            "Iteration  388: Training Loss 0.00000246, Validation Loss 0.00002093\n",
            "Iteration  389: Training Loss 0.00000246, Validation Loss 0.00002093\n",
            "Iteration  390: Training Loss 0.00000245, Validation Loss 0.00002094\n",
            "Iteration  391: Training Loss 0.00000245, Validation Loss 0.00002094\n",
            "Iteration  392: Training Loss 0.00000245, Validation Loss 0.00002095\n",
            "Iteration  393: Training Loss 0.00000245, Validation Loss 0.00002095\n",
            "Iteration  394: Training Loss 0.00000245, Validation Loss 0.00002096\n",
            "Iteration  395: Training Loss 0.00000245, Validation Loss 0.00002096\n",
            "Iteration  396: Training Loss 0.00000245, Validation Loss 0.00002096\n",
            "Iteration  397: Training Loss 0.00000245, Validation Loss 0.00002097\n",
            "Iteration  398: Training Loss 0.00000245, Validation Loss 0.00002097\n",
            "Iteration  399: Training Loss 0.00000245, Validation Loss 0.00002098\n",
            "Iteration  400: Training Loss 0.00000245, Validation Loss 0.00002098\n",
            "Iteration  401: Training Loss 0.00000244, Validation Loss 0.00002099\n",
            "Iteration  402: Training Loss 0.00000244, Validation Loss 0.00002099\n",
            "Iteration  403: Training Loss 0.00000244, Validation Loss 0.00002100\n",
            "Iteration  404: Training Loss 0.00000244, Validation Loss 0.00002100\n",
            "Iteration  405: Training Loss 0.00000244, Validation Loss 0.00002100\n",
            "Iteration  406: Training Loss 0.00000244, Validation Loss 0.00002101\n",
            "Iteration  407: Training Loss 0.00000244, Validation Loss 0.00002101\n",
            "Iteration  408: Training Loss 0.00000244, Validation Loss 0.00002102\n",
            "Iteration  409: Training Loss 0.00000244, Validation Loss 0.00002102\n",
            "Iteration  410: Training Loss 0.00000244, Validation Loss 0.00002103\n",
            "Iteration  411: Training Loss 0.00000244, Validation Loss 0.00002103\n",
            "Iteration  412: Training Loss 0.00000243, Validation Loss 0.00002104\n",
            "Iteration  413: Training Loss 0.00000243, Validation Loss 0.00002104\n",
            "Iteration  414: Training Loss 0.00000243, Validation Loss 0.00002104\n",
            "Iteration  415: Training Loss 0.00000243, Validation Loss 0.00002105\n",
            "Iteration  416: Training Loss 0.00000243, Validation Loss 0.00002105\n",
            "Iteration  417: Training Loss 0.00000243, Validation Loss 0.00002106\n",
            "Iteration  418: Training Loss 0.00000243, Validation Loss 0.00002106\n",
            "Iteration  419: Training Loss 0.00000243, Validation Loss 0.00002107\n",
            "Iteration  420: Training Loss 0.00000243, Validation Loss 0.00002107\n",
            "Iteration  421: Training Loss 0.00000243, Validation Loss 0.00002107\n",
            "Iteration  422: Training Loss 0.00000243, Validation Loss 0.00002108\n",
            "Iteration  423: Training Loss 0.00000242, Validation Loss 0.00002108\n",
            "Iteration  424: Training Loss 0.00000242, Validation Loss 0.00002109\n",
            "Iteration  425: Training Loss 0.00000242, Validation Loss 0.00002109\n",
            "Iteration  426: Training Loss 0.00000242, Validation Loss 0.00002110\n",
            "Iteration  427: Training Loss 0.00000242, Validation Loss 0.00002110\n",
            "Iteration  428: Training Loss 0.00000242, Validation Loss 0.00002110\n",
            "Iteration  429: Training Loss 0.00000242, Validation Loss 0.00002111\n",
            "Iteration  430: Training Loss 0.00000242, Validation Loss 0.00002111\n",
            "Iteration  431: Training Loss 0.00000242, Validation Loss 0.00002112\n",
            "Iteration  432: Training Loss 0.00000242, Validation Loss 0.00002112\n",
            "Iteration  433: Training Loss 0.00000242, Validation Loss 0.00002113\n",
            "Iteration  434: Training Loss 0.00000241, Validation Loss 0.00002113\n",
            "Iteration  435: Training Loss 0.00000241, Validation Loss 0.00002113\n",
            "Iteration  436: Training Loss 0.00000241, Validation Loss 0.00002114\n",
            "Iteration  437: Training Loss 0.00000241, Validation Loss 0.00002114\n",
            "Iteration  438: Training Loss 0.00000241, Validation Loss 0.00002115\n",
            "Iteration  439: Training Loss 0.00000241, Validation Loss 0.00002115\n",
            "Iteration  440: Training Loss 0.00000241, Validation Loss 0.00002116\n",
            "Iteration  441: Training Loss 0.00000241, Validation Loss 0.00002116\n",
            "Iteration  442: Training Loss 0.00000241, Validation Loss 0.00002116\n",
            "Iteration  443: Training Loss 0.00000241, Validation Loss 0.00002117\n",
            "Iteration  444: Training Loss 0.00000241, Validation Loss 0.00002117\n",
            "Iteration  445: Training Loss 0.00000240, Validation Loss 0.00002118\n",
            "Iteration  446: Training Loss 0.00000240, Validation Loss 0.00002118\n",
            "Iteration  447: Training Loss 0.00000240, Validation Loss 0.00002118\n",
            "Iteration  448: Training Loss 0.00000240, Validation Loss 0.00002119\n",
            "Iteration  449: Training Loss 0.00000240, Validation Loss 0.00002119\n",
            "Iteration  450: Training Loss 0.00000240, Validation Loss 0.00002120\n",
            "Iteration  451: Training Loss 0.00000240, Validation Loss 0.00002120\n",
            "Iteration  452: Training Loss 0.00000240, Validation Loss 0.00002120\n",
            "Iteration  453: Training Loss 0.00000240, Validation Loss 0.00002121\n",
            "Iteration  454: Training Loss 0.00000240, Validation Loss 0.00002121\n",
            "Iteration  455: Training Loss 0.00000240, Validation Loss 0.00002122\n",
            "Iteration  456: Training Loss 0.00000240, Validation Loss 0.00002122\n",
            "Iteration  457: Training Loss 0.00000239, Validation Loss 0.00002123\n",
            "Iteration  458: Training Loss 0.00000239, Validation Loss 0.00002123\n",
            "Iteration  459: Training Loss 0.00000239, Validation Loss 0.00002123\n",
            "Iteration  460: Training Loss 0.00000239, Validation Loss 0.00002124\n",
            "Iteration  461: Training Loss 0.00000239, Validation Loss 0.00002124\n",
            "Iteration  462: Training Loss 0.00000239, Validation Loss 0.00002125\n",
            "Iteration  463: Training Loss 0.00000239, Validation Loss 0.00002125\n",
            "Iteration  464: Training Loss 0.00000239, Validation Loss 0.00002125\n",
            "Iteration  465: Training Loss 0.00000239, Validation Loss 0.00002126\n",
            "Iteration  466: Training Loss 0.00000239, Validation Loss 0.00002126\n",
            "Iteration  467: Training Loss 0.00000239, Validation Loss 0.00002127\n",
            "Iteration  468: Training Loss 0.00000239, Validation Loss 0.00002127\n",
            "Iteration  469: Training Loss 0.00000238, Validation Loss 0.00002127\n",
            "Iteration  470: Training Loss 0.00000238, Validation Loss 0.00002128\n",
            "Iteration  471: Training Loss 0.00000238, Validation Loss 0.00002128\n",
            "Iteration  472: Training Loss 0.00000238, Validation Loss 0.00002129\n",
            "Iteration  473: Training Loss 0.00000238, Validation Loss 0.00002129\n",
            "Iteration  474: Training Loss 0.00000238, Validation Loss 0.00002129\n",
            "Iteration  475: Training Loss 0.00000238, Validation Loss 0.00002130\n",
            "Iteration  476: Training Loss 0.00000238, Validation Loss 0.00002130\n",
            "Iteration  477: Training Loss 0.00000238, Validation Loss 0.00002131\n",
            "Iteration  478: Training Loss 0.00000238, Validation Loss 0.00002131\n",
            "Iteration  479: Training Loss 0.00000238, Validation Loss 0.00002131\n",
            "Iteration  480: Training Loss 0.00000238, Validation Loss 0.00002132\n",
            "Iteration  481: Training Loss 0.00000237, Validation Loss 0.00002132\n",
            "Iteration  482: Training Loss 0.00000237, Validation Loss 0.00002132\n",
            "Iteration  483: Training Loss 0.00000237, Validation Loss 0.00002133\n",
            "Iteration  484: Training Loss 0.00000237, Validation Loss 0.00002133\n",
            "Iteration  485: Training Loss 0.00000237, Validation Loss 0.00002134\n",
            "Iteration  486: Training Loss 0.00000237, Validation Loss 0.00002134\n",
            "Iteration  487: Training Loss 0.00000237, Validation Loss 0.00002134\n",
            "Iteration  488: Training Loss 0.00000237, Validation Loss 0.00002135\n",
            "Iteration  489: Training Loss 0.00000237, Validation Loss 0.00002135\n",
            "Iteration  490: Training Loss 0.00000237, Validation Loss 0.00002136\n",
            "Iteration  491: Training Loss 0.00000237, Validation Loss 0.00002136\n",
            "Iteration  492: Training Loss 0.00000237, Validation Loss 0.00002136\n",
            "Iteration  493: Training Loss 0.00000237, Validation Loss 0.00002137\n",
            "Iteration  494: Training Loss 0.00000236, Validation Loss 0.00002137\n",
            "Iteration  495: Training Loss 0.00000236, Validation Loss 0.00002137\n",
            "Iteration  496: Training Loss 0.00000236, Validation Loss 0.00002138\n",
            "Iteration  497: Training Loss 0.00000236, Validation Loss 0.00002138\n",
            "Iteration  498: Training Loss 0.00000236, Validation Loss 0.00002139\n",
            "Iteration  499: Training Loss 0.00000236, Validation Loss 0.00002139\n",
            "Iteration  500: Training Loss 0.00000236, Validation Loss 0.00002139\n",
            "Iteration  501: Training Loss 0.00000236, Validation Loss 0.00002140\n",
            "Iteration  502: Training Loss 0.00000236, Validation Loss 0.00002140\n",
            "Iteration  503: Training Loss 0.00000236, Validation Loss 0.00002140\n",
            "Iteration  504: Training Loss 0.00000236, Validation Loss 0.00002141\n",
            "Iteration  505: Training Loss 0.00000236, Validation Loss 0.00002141\n",
            "Iteration  506: Training Loss 0.00000235, Validation Loss 0.00002142\n",
            "Iteration  507: Training Loss 0.00000235, Validation Loss 0.00002142\n",
            "Iteration  508: Training Loss 0.00000235, Validation Loss 0.00002142\n",
            "Iteration  509: Training Loss 0.00000235, Validation Loss 0.00002143\n",
            "Iteration  510: Training Loss 0.00000235, Validation Loss 0.00002143\n",
            "Iteration  511: Training Loss 0.00000235, Validation Loss 0.00002143\n",
            "Iteration  512: Training Loss 0.00000235, Validation Loss 0.00002144\n",
            "Iteration  513: Training Loss 0.00000235, Validation Loss 0.00002144\n",
            "Iteration  514: Training Loss 0.00000235, Validation Loss 0.00002145\n",
            "Iteration  515: Training Loss 0.00000235, Validation Loss 0.00002145\n",
            "Iteration  516: Training Loss 0.00000235, Validation Loss 0.00002145\n",
            "Iteration  517: Training Loss 0.00000235, Validation Loss 0.00002146\n",
            "Iteration  518: Training Loss 0.00000235, Validation Loss 0.00002146\n",
            "Iteration  519: Training Loss 0.00000235, Validation Loss 0.00002146\n",
            "Iteration  520: Training Loss 0.00000234, Validation Loss 0.00002147\n",
            "Iteration  521: Training Loss 0.00000234, Validation Loss 0.00002147\n",
            "Iteration  522: Training Loss 0.00000234, Validation Loss 0.00002147\n",
            "Iteration  523: Training Loss 0.00000234, Validation Loss 0.00002148\n",
            "Iteration  524: Training Loss 0.00000234, Validation Loss 0.00002148\n",
            "Iteration  525: Training Loss 0.00000234, Validation Loss 0.00002149\n",
            "Iteration  526: Training Loss 0.00000234, Validation Loss 0.00002149\n",
            "Iteration  527: Training Loss 0.00000234, Validation Loss 0.00002149\n",
            "Iteration  528: Training Loss 0.00000234, Validation Loss 0.00002150\n",
            "Iteration  529: Training Loss 0.00000234, Validation Loss 0.00002150\n",
            "Iteration  530: Training Loss 0.00000234, Validation Loss 0.00002150\n",
            "Iteration  531: Training Loss 0.00000234, Validation Loss 0.00002151\n",
            "Iteration  532: Training Loss 0.00000234, Validation Loss 0.00002151\n",
            "Iteration  533: Training Loss 0.00000233, Validation Loss 0.00002151\n",
            "Iteration  534: Training Loss 0.00000233, Validation Loss 0.00002152\n",
            "Iteration  535: Training Loss 0.00000233, Validation Loss 0.00002152\n",
            "Iteration  536: Training Loss 0.00000233, Validation Loss 0.00002152\n",
            "Iteration  537: Training Loss 0.00000233, Validation Loss 0.00002153\n",
            "Iteration  538: Training Loss 0.00000233, Validation Loss 0.00002153\n",
            "Iteration  539: Training Loss 0.00000233, Validation Loss 0.00002154\n",
            "Iteration  540: Training Loss 0.00000233, Validation Loss 0.00002154\n",
            "Iteration  541: Training Loss 0.00000233, Validation Loss 0.00002154\n",
            "Iteration  542: Training Loss 0.00000233, Validation Loss 0.00002155\n",
            "Iteration  543: Training Loss 0.00000233, Validation Loss 0.00002155\n",
            "Iteration  544: Training Loss 0.00000233, Validation Loss 0.00002155\n",
            "Iteration  545: Training Loss 0.00000233, Validation Loss 0.00002156\n",
            "Iteration  546: Training Loss 0.00000233, Validation Loss 0.00002156\n",
            "Iteration  547: Training Loss 0.00000232, Validation Loss 0.00002156\n",
            "Iteration  548: Training Loss 0.00000232, Validation Loss 0.00002157\n",
            "Iteration  549: Training Loss 0.00000232, Validation Loss 0.00002157\n",
            "Iteration  550: Training Loss 0.00000232, Validation Loss 0.00002157\n",
            "Iteration  551: Training Loss 0.00000232, Validation Loss 0.00002158\n",
            "Iteration  552: Training Loss 0.00000232, Validation Loss 0.00002158\n",
            "Iteration  553: Training Loss 0.00000232, Validation Loss 0.00002158\n",
            "Iteration  554: Training Loss 0.00000232, Validation Loss 0.00002159\n",
            "Iteration  555: Training Loss 0.00000232, Validation Loss 0.00002159\n",
            "Iteration  556: Training Loss 0.00000232, Validation Loss 0.00002159\n",
            "Iteration  557: Training Loss 0.00000232, Validation Loss 0.00002160\n",
            "Iteration  558: Training Loss 0.00000232, Validation Loss 0.00002160\n",
            "Iteration  559: Training Loss 0.00000232, Validation Loss 0.00002160\n",
            "Iteration  560: Training Loss 0.00000232, Validation Loss 0.00002161\n",
            "Iteration  561: Training Loss 0.00000232, Validation Loss 0.00002161\n",
            "Iteration  562: Training Loss 0.00000231, Validation Loss 0.00002161\n",
            "Iteration  563: Training Loss 0.00000231, Validation Loss 0.00002162\n",
            "Iteration  564: Training Loss 0.00000231, Validation Loss 0.00002162\n",
            "Iteration  565: Training Loss 0.00000231, Validation Loss 0.00002162\n",
            "Iteration  566: Training Loss 0.00000231, Validation Loss 0.00002163\n",
            "Iteration  567: Training Loss 0.00000231, Validation Loss 0.00002163\n",
            "Iteration  568: Training Loss 0.00000231, Validation Loss 0.00002163\n",
            "Iteration  569: Training Loss 0.00000231, Validation Loss 0.00002164\n",
            "Iteration  570: Training Loss 0.00000231, Validation Loss 0.00002164\n",
            "Iteration  571: Training Loss 0.00000231, Validation Loss 0.00002164\n",
            "Iteration  572: Training Loss 0.00000231, Validation Loss 0.00002165\n",
            "Iteration  573: Training Loss 0.00000231, Validation Loss 0.00002165\n",
            "Iteration  574: Training Loss 0.00000231, Validation Loss 0.00002165\n",
            "Iteration  575: Training Loss 0.00000231, Validation Loss 0.00002166\n",
            "Iteration  576: Training Loss 0.00000231, Validation Loss 0.00002166\n",
            "Iteration  577: Training Loss 0.00000230, Validation Loss 0.00002166\n",
            "Iteration  578: Training Loss 0.00000230, Validation Loss 0.00002167\n",
            "Iteration  579: Training Loss 0.00000230, Validation Loss 0.00002167\n",
            "Iteration  580: Training Loss 0.00000230, Validation Loss 0.00002167\n",
            "Iteration  581: Training Loss 0.00000230, Validation Loss 0.00002168\n",
            "Iteration  582: Training Loss 0.00000230, Validation Loss 0.00002168\n",
            "Iteration  583: Training Loss 0.00000230, Validation Loss 0.00002168\n",
            "Iteration  584: Training Loss 0.00000230, Validation Loss 0.00002169\n",
            "Iteration  585: Training Loss 0.00000230, Validation Loss 0.00002169\n",
            "Iteration  586: Training Loss 0.00000230, Validation Loss 0.00002169\n",
            "Iteration  587: Training Loss 0.00000230, Validation Loss 0.00002170\n",
            "Iteration  588: Training Loss 0.00000230, Validation Loss 0.00002170\n",
            "Iteration  589: Training Loss 0.00000230, Validation Loss 0.00002170\n",
            "Iteration  590: Training Loss 0.00000230, Validation Loss 0.00002171\n",
            "Iteration  591: Training Loss 0.00000230, Validation Loss 0.00002171\n",
            "Iteration  592: Training Loss 0.00000229, Validation Loss 0.00002171\n",
            "Iteration  593: Training Loss 0.00000229, Validation Loss 0.00002171\n",
            "Iteration  594: Training Loss 0.00000229, Validation Loss 0.00002172\n",
            "Iteration  595: Training Loss 0.00000229, Validation Loss 0.00002172\n",
            "Iteration  596: Training Loss 0.00000229, Validation Loss 0.00002172\n",
            "Iteration  597: Training Loss 0.00000229, Validation Loss 0.00002173\n",
            "Iteration  598: Training Loss 0.00000229, Validation Loss 0.00002173\n",
            "Iteration  599: Training Loss 0.00000229, Validation Loss 0.00002173\n",
            "Iteration  600: Training Loss 0.00000229, Validation Loss 0.00002174\n",
            "Iteration  601: Training Loss 0.00000229, Validation Loss 0.00002174\n",
            "Iteration  602: Training Loss 0.00000229, Validation Loss 0.00002174\n",
            "Iteration  603: Training Loss 0.00000229, Validation Loss 0.00002175\n",
            "Iteration  604: Training Loss 0.00000229, Validation Loss 0.00002175\n",
            "Iteration  605: Training Loss 0.00000229, Validation Loss 0.00002175\n",
            "Iteration  606: Training Loss 0.00000229, Validation Loss 0.00002176\n",
            "Iteration  607: Training Loss 0.00000229, Validation Loss 0.00002176\n",
            "Iteration  608: Training Loss 0.00000228, Validation Loss 0.00002176\n",
            "Iteration  609: Training Loss 0.00000228, Validation Loss 0.00002176\n",
            "Iteration  610: Training Loss 0.00000228, Validation Loss 0.00002177\n",
            "Iteration  611: Training Loss 0.00000228, Validation Loss 0.00002177\n",
            "Iteration  612: Training Loss 0.00000228, Validation Loss 0.00002177\n",
            "Iteration  613: Training Loss 0.00000228, Validation Loss 0.00002178\n",
            "Iteration  614: Training Loss 0.00000228, Validation Loss 0.00002178\n",
            "Iteration  615: Training Loss 0.00000228, Validation Loss 0.00002178\n",
            "Iteration  616: Training Loss 0.00000228, Validation Loss 0.00002179\n",
            "Iteration  617: Training Loss 0.00000228, Validation Loss 0.00002179\n",
            "Iteration  618: Training Loss 0.00000228, Validation Loss 0.00002179\n",
            "Iteration  619: Training Loss 0.00000228, Validation Loss 0.00002179\n",
            "Iteration  620: Training Loss 0.00000228, Validation Loss 0.00002180\n",
            "Iteration  621: Training Loss 0.00000228, Validation Loss 0.00002180\n",
            "Iteration  622: Training Loss 0.00000228, Validation Loss 0.00002180\n",
            "Iteration  623: Training Loss 0.00000228, Validation Loss 0.00002181\n",
            "Iteration  624: Training Loss 0.00000228, Validation Loss 0.00002181\n",
            "Iteration  625: Training Loss 0.00000227, Validation Loss 0.00002181\n",
            "Iteration  626: Training Loss 0.00000227, Validation Loss 0.00002182\n",
            "Iteration  627: Training Loss 0.00000227, Validation Loss 0.00002182\n",
            "Iteration  628: Training Loss 0.00000227, Validation Loss 0.00002182\n",
            "Iteration  629: Training Loss 0.00000227, Validation Loss 0.00002182\n",
            "Iteration  630: Training Loss 0.00000227, Validation Loss 0.00002183\n",
            "Iteration  631: Training Loss 0.00000227, Validation Loss 0.00002183\n",
            "Iteration  632: Training Loss 0.00000227, Validation Loss 0.00002183\n",
            "Iteration  633: Training Loss 0.00000227, Validation Loss 0.00002184\n",
            "Iteration  634: Training Loss 0.00000227, Validation Loss 0.00002184\n",
            "Iteration  635: Training Loss 0.00000227, Validation Loss 0.00002184\n",
            "Iteration  636: Training Loss 0.00000227, Validation Loss 0.00002185\n",
            "Iteration  637: Training Loss 0.00000227, Validation Loss 0.00002185\n",
            "Iteration  638: Training Loss 0.00000227, Validation Loss 0.00002185\n",
            "Iteration  639: Training Loss 0.00000227, Validation Loss 0.00002185\n",
            "Iteration  640: Training Loss 0.00000227, Validation Loss 0.00002186\n",
            "Iteration  641: Training Loss 0.00000227, Validation Loss 0.00002186\n",
            "Iteration  642: Training Loss 0.00000226, Validation Loss 0.00002186\n",
            "Iteration  643: Training Loss 0.00000226, Validation Loss 0.00002187\n",
            "Iteration  644: Training Loss 0.00000226, Validation Loss 0.00002187\n",
            "Iteration  645: Training Loss 0.00000226, Validation Loss 0.00002187\n",
            "Iteration  646: Training Loss 0.00000226, Validation Loss 0.00002187\n",
            "Iteration  647: Training Loss 0.00000226, Validation Loss 0.00002188\n",
            "Iteration  648: Training Loss 0.00000226, Validation Loss 0.00002188\n",
            "Iteration  649: Training Loss 0.00000226, Validation Loss 0.00002188\n",
            "Iteration  650: Training Loss 0.00000226, Validation Loss 0.00002189\n",
            "Iteration  651: Training Loss 0.00000226, Validation Loss 0.00002189\n",
            "Iteration  652: Training Loss 0.00000226, Validation Loss 0.00002189\n",
            "Iteration  653: Training Loss 0.00000226, Validation Loss 0.00002189\n",
            "Iteration  654: Training Loss 0.00000226, Validation Loss 0.00002190\n",
            "Iteration  655: Training Loss 0.00000226, Validation Loss 0.00002190\n",
            "Iteration  656: Training Loss 0.00000226, Validation Loss 0.00002190\n",
            "Iteration  657: Training Loss 0.00000226, Validation Loss 0.00002191\n",
            "Iteration  658: Training Loss 0.00000226, Validation Loss 0.00002191\n",
            "Iteration  659: Training Loss 0.00000225, Validation Loss 0.00002191\n",
            "Iteration  660: Training Loss 0.00000225, Validation Loss 0.00002191\n",
            "Iteration  661: Training Loss 0.00000225, Validation Loss 0.00002192\n",
            "Iteration  662: Training Loss 0.00000225, Validation Loss 0.00002192\n",
            "Iteration  663: Training Loss 0.00000225, Validation Loss 0.00002192\n",
            "Iteration  664: Training Loss 0.00000225, Validation Loss 0.00002192\n",
            "Iteration  665: Training Loss 0.00000225, Validation Loss 0.00002193\n",
            "Iteration  666: Training Loss 0.00000225, Validation Loss 0.00002193\n",
            "Iteration  667: Training Loss 0.00000225, Validation Loss 0.00002193\n",
            "Iteration  668: Training Loss 0.00000225, Validation Loss 0.00002194\n",
            "Iteration  669: Training Loss 0.00000225, Validation Loss 0.00002194\n",
            "Iteration  670: Training Loss 0.00000225, Validation Loss 0.00002194\n",
            "Iteration  671: Training Loss 0.00000225, Validation Loss 0.00002194\n",
            "Iteration  672: Training Loss 0.00000225, Validation Loss 0.00002195\n",
            "Iteration  673: Training Loss 0.00000225, Validation Loss 0.00002195\n",
            "Iteration  674: Training Loss 0.00000225, Validation Loss 0.00002195\n",
            "Iteration  675: Training Loss 0.00000225, Validation Loss 0.00002195\n",
            "Iteration  676: Training Loss 0.00000225, Validation Loss 0.00002196\n",
            "Iteration  677: Training Loss 0.00000225, Validation Loss 0.00002196\n",
            "Iteration  678: Training Loss 0.00000224, Validation Loss 0.00002196\n",
            "Iteration  679: Training Loss 0.00000224, Validation Loss 0.00002197\n",
            "Iteration  680: Training Loss 0.00000224, Validation Loss 0.00002197\n",
            "Iteration  681: Training Loss 0.00000224, Validation Loss 0.00002197\n",
            "Iteration  682: Training Loss 0.00000224, Validation Loss 0.00002197\n",
            "Iteration  683: Training Loss 0.00000224, Validation Loss 0.00002198\n",
            "Iteration  684: Training Loss 0.00000224, Validation Loss 0.00002198\n",
            "Iteration  685: Training Loss 0.00000224, Validation Loss 0.00002198\n",
            "Iteration  686: Training Loss 0.00000224, Validation Loss 0.00002198\n",
            "Iteration  687: Training Loss 0.00000224, Validation Loss 0.00002199\n",
            "Iteration  688: Training Loss 0.00000224, Validation Loss 0.00002199\n",
            "Iteration  689: Training Loss 0.00000224, Validation Loss 0.00002199\n",
            "Iteration  690: Training Loss 0.00000224, Validation Loss 0.00002199\n",
            "Iteration  691: Training Loss 0.00000224, Validation Loss 0.00002200\n",
            "Iteration  692: Training Loss 0.00000224, Validation Loss 0.00002200\n",
            "Iteration  693: Training Loss 0.00000224, Validation Loss 0.00002200\n",
            "Iteration  694: Training Loss 0.00000224, Validation Loss 0.00002200\n",
            "Iteration  695: Training Loss 0.00000224, Validation Loss 0.00002201\n",
            "Iteration  696: Training Loss 0.00000224, Validation Loss 0.00002201\n",
            "Iteration  697: Training Loss 0.00000223, Validation Loss 0.00002201\n",
            "Iteration  698: Training Loss 0.00000223, Validation Loss 0.00002202\n",
            "Iteration  699: Training Loss 0.00000223, Validation Loss 0.00002202\n",
            "Iteration  700: Training Loss 0.00000223, Validation Loss 0.00002202\n",
            "Iteration  701: Training Loss 0.00000223, Validation Loss 0.00002202\n",
            "Iteration  702: Training Loss 0.00000223, Validation Loss 0.00002203\n",
            "Iteration  703: Training Loss 0.00000223, Validation Loss 0.00002203\n",
            "Iteration  704: Training Loss 0.00000223, Validation Loss 0.00002203\n",
            "Iteration  705: Training Loss 0.00000223, Validation Loss 0.00002203\n",
            "Iteration  706: Training Loss 0.00000223, Validation Loss 0.00002204\n",
            "Iteration  707: Training Loss 0.00000223, Validation Loss 0.00002204\n",
            "Iteration  708: Training Loss 0.00000223, Validation Loss 0.00002204\n",
            "Iteration  709: Training Loss 0.00000223, Validation Loss 0.00002204\n",
            "Iteration  710: Training Loss 0.00000223, Validation Loss 0.00002205\n",
            "Iteration  711: Training Loss 0.00000223, Validation Loss 0.00002205\n",
            "Iteration  712: Training Loss 0.00000223, Validation Loss 0.00002205\n",
            "Iteration  713: Training Loss 0.00000223, Validation Loss 0.00002205\n",
            "Iteration  714: Training Loss 0.00000223, Validation Loss 0.00002206\n",
            "Iteration  715: Training Loss 0.00000223, Validation Loss 0.00002206\n",
            "Iteration  716: Training Loss 0.00000223, Validation Loss 0.00002206\n",
            "Iteration  717: Training Loss 0.00000222, Validation Loss 0.00002206\n",
            "Iteration  718: Training Loss 0.00000222, Validation Loss 0.00002207\n",
            "Iteration  719: Training Loss 0.00000222, Validation Loss 0.00002207\n",
            "Iteration  720: Training Loss 0.00000222, Validation Loss 0.00002207\n",
            "Iteration  721: Training Loss 0.00000222, Validation Loss 0.00002207\n",
            "Iteration  722: Training Loss 0.00000222, Validation Loss 0.00002208\n",
            "Iteration  723: Training Loss 0.00000222, Validation Loss 0.00002208\n",
            "Iteration  724: Training Loss 0.00000222, Validation Loss 0.00002208\n",
            "Iteration  725: Training Loss 0.00000222, Validation Loss 0.00002208\n",
            "Iteration  726: Training Loss 0.00000222, Validation Loss 0.00002209\n",
            "Iteration  727: Training Loss 0.00000222, Validation Loss 0.00002209\n",
            "Iteration  728: Training Loss 0.00000222, Validation Loss 0.00002209\n",
            "Iteration  729: Training Loss 0.00000222, Validation Loss 0.00002209\n",
            "Iteration  730: Training Loss 0.00000222, Validation Loss 0.00002210\n",
            "Iteration  731: Training Loss 0.00000222, Validation Loss 0.00002210\n",
            "Iteration  732: Training Loss 0.00000222, Validation Loss 0.00002210\n",
            "Iteration  733: Training Loss 0.00000222, Validation Loss 0.00002210\n",
            "Iteration  734: Training Loss 0.00000222, Validation Loss 0.00002211\n",
            "Iteration  735: Training Loss 0.00000222, Validation Loss 0.00002211\n",
            "Iteration  736: Training Loss 0.00000222, Validation Loss 0.00002211\n",
            "Iteration  737: Training Loss 0.00000222, Validation Loss 0.00002211\n",
            "Iteration  738: Training Loss 0.00000221, Validation Loss 0.00002211\n",
            "Iteration  739: Training Loss 0.00000221, Validation Loss 0.00002212\n",
            "Iteration  740: Training Loss 0.00000221, Validation Loss 0.00002212\n",
            "Iteration  741: Training Loss 0.00000221, Validation Loss 0.00002212\n",
            "Iteration  742: Training Loss 0.00000221, Validation Loss 0.00002212\n",
            "Iteration  743: Training Loss 0.00000221, Validation Loss 0.00002213\n",
            "Iteration  744: Training Loss 0.00000221, Validation Loss 0.00002213\n",
            "Iteration  745: Training Loss 0.00000221, Validation Loss 0.00002213\n",
            "Iteration  746: Training Loss 0.00000221, Validation Loss 0.00002213\n",
            "Iteration  747: Training Loss 0.00000221, Validation Loss 0.00002214\n",
            "Iteration  748: Training Loss 0.00000221, Validation Loss 0.00002214\n",
            "Iteration  749: Training Loss 0.00000221, Validation Loss 0.00002214\n",
            "Iteration  750: Training Loss 0.00000221, Validation Loss 0.00002214\n",
            "Iteration  751: Training Loss 0.00000221, Validation Loss 0.00002215\n",
            "Iteration  752: Training Loss 0.00000221, Validation Loss 0.00002215\n",
            "Iteration  753: Training Loss 0.00000221, Validation Loss 0.00002215\n",
            "Iteration  754: Training Loss 0.00000221, Validation Loss 0.00002215\n",
            "Iteration  755: Training Loss 0.00000221, Validation Loss 0.00002215\n",
            "Iteration  756: Training Loss 0.00000221, Validation Loss 0.00002216\n",
            "Iteration  757: Training Loss 0.00000221, Validation Loss 0.00002216\n",
            "Iteration  758: Training Loss 0.00000221, Validation Loss 0.00002216\n",
            "Iteration  759: Training Loss 0.00000220, Validation Loss 0.00002216\n",
            "Iteration  760: Training Loss 0.00000220, Validation Loss 0.00002217\n",
            "Iteration  761: Training Loss 0.00000220, Validation Loss 0.00002217\n",
            "Iteration  762: Training Loss 0.00000220, Validation Loss 0.00002217\n",
            "Iteration  763: Training Loss 0.00000220, Validation Loss 0.00002217\n",
            "Iteration  764: Training Loss 0.00000220, Validation Loss 0.00002218\n",
            "Iteration  765: Training Loss 0.00000220, Validation Loss 0.00002218\n",
            "Iteration  766: Training Loss 0.00000220, Validation Loss 0.00002218\n",
            "Iteration  767: Training Loss 0.00000220, Validation Loss 0.00002218\n",
            "Iteration  768: Training Loss 0.00000220, Validation Loss 0.00002218\n",
            "Iteration  769: Training Loss 0.00000220, Validation Loss 0.00002219\n",
            "Iteration  770: Training Loss 0.00000220, Validation Loss 0.00002219\n",
            "Iteration  771: Training Loss 0.00000220, Validation Loss 0.00002219\n",
            "Iteration  772: Training Loss 0.00000220, Validation Loss 0.00002219\n",
            "Iteration  773: Training Loss 0.00000220, Validation Loss 0.00002220\n",
            "Iteration  774: Training Loss 0.00000220, Validation Loss 0.00002220\n",
            "Iteration  775: Training Loss 0.00000220, Validation Loss 0.00002220\n",
            "Iteration  776: Training Loss 0.00000220, Validation Loss 0.00002220\n",
            "Iteration  777: Training Loss 0.00000220, Validation Loss 0.00002220\n",
            "Iteration  778: Training Loss 0.00000220, Validation Loss 0.00002221\n",
            "Iteration  779: Training Loss 0.00000220, Validation Loss 0.00002221\n",
            "Iteration  780: Training Loss 0.00000220, Validation Loss 0.00002221\n",
            "Iteration  781: Training Loss 0.00000220, Validation Loss 0.00002221\n",
            "Iteration  782: Training Loss 0.00000219, Validation Loss 0.00002222\n",
            "Iteration  783: Training Loss 0.00000219, Validation Loss 0.00002222\n",
            "Iteration  784: Training Loss 0.00000219, Validation Loss 0.00002222\n",
            "Iteration  785: Training Loss 0.00000219, Validation Loss 0.00002222\n",
            "Iteration  786: Training Loss 0.00000219, Validation Loss 0.00002222\n",
            "Iteration  787: Training Loss 0.00000219, Validation Loss 0.00002223\n",
            "Iteration  788: Training Loss 0.00000219, Validation Loss 0.00002223\n",
            "Iteration  789: Training Loss 0.00000219, Validation Loss 0.00002223\n",
            "Iteration  790: Training Loss 0.00000219, Validation Loss 0.00002223\n",
            "Iteration  791: Training Loss 0.00000219, Validation Loss 0.00002224\n",
            "Iteration  792: Training Loss 0.00000219, Validation Loss 0.00002224\n",
            "Iteration  793: Training Loss 0.00000219, Validation Loss 0.00002224\n",
            "Iteration  794: Training Loss 0.00000219, Validation Loss 0.00002224\n",
            "Iteration  795: Training Loss 0.00000219, Validation Loss 0.00002224\n",
            "Iteration  796: Training Loss 0.00000219, Validation Loss 0.00002225\n",
            "Iteration  797: Training Loss 0.00000219, Validation Loss 0.00002225\n",
            "Iteration  798: Training Loss 0.00000219, Validation Loss 0.00002225\n",
            "Iteration  799: Training Loss 0.00000219, Validation Loss 0.00002225\n",
            "Iteration  800: Training Loss 0.00000219, Validation Loss 0.00002225\n",
            "Iteration  801: Training Loss 0.00000219, Validation Loss 0.00002226\n",
            "Iteration  802: Training Loss 0.00000219, Validation Loss 0.00002226\n",
            "Iteration  803: Training Loss 0.00000219, Validation Loss 0.00002226\n",
            "Iteration  804: Training Loss 0.00000219, Validation Loss 0.00002226\n",
            "Iteration  805: Training Loss 0.00000219, Validation Loss 0.00002227\n",
            "Iteration  806: Training Loss 0.00000218, Validation Loss 0.00002227\n",
            "Iteration  807: Training Loss 0.00000218, Validation Loss 0.00002227\n",
            "Iteration  808: Training Loss 0.00000218, Validation Loss 0.00002227\n",
            "Iteration  809: Training Loss 0.00000218, Validation Loss 0.00002227\n",
            "Iteration  810: Training Loss 0.00000218, Validation Loss 0.00002228\n",
            "Iteration  811: Training Loss 0.00000218, Validation Loss 0.00002228\n",
            "Iteration  812: Training Loss 0.00000218, Validation Loss 0.00002228\n",
            "Iteration  813: Training Loss 0.00000218, Validation Loss 0.00002228\n",
            "Iteration  814: Training Loss 0.00000218, Validation Loss 0.00002228\n",
            "Iteration  815: Training Loss 0.00000218, Validation Loss 0.00002229\n",
            "Iteration  816: Training Loss 0.00000218, Validation Loss 0.00002229\n",
            "Iteration  817: Training Loss 0.00000218, Validation Loss 0.00002229\n",
            "Iteration  818: Training Loss 0.00000218, Validation Loss 0.00002229\n",
            "Iteration  819: Training Loss 0.00000218, Validation Loss 0.00002229\n",
            "Iteration  820: Training Loss 0.00000218, Validation Loss 0.00002230\n",
            "Iteration  821: Training Loss 0.00000218, Validation Loss 0.00002230\n",
            "Iteration  822: Training Loss 0.00000218, Validation Loss 0.00002230\n",
            "Iteration  823: Training Loss 0.00000218, Validation Loss 0.00002230\n",
            "Iteration  824: Training Loss 0.00000218, Validation Loss 0.00002230\n",
            "Iteration  825: Training Loss 0.00000218, Validation Loss 0.00002231\n",
            "Iteration  826: Training Loss 0.00000218, Validation Loss 0.00002231\n",
            "Iteration  827: Training Loss 0.00000218, Validation Loss 0.00002231\n",
            "Iteration  828: Training Loss 0.00000218, Validation Loss 0.00002231\n",
            "Iteration  829: Training Loss 0.00000218, Validation Loss 0.00002231\n",
            "Iteration  830: Training Loss 0.00000218, Validation Loss 0.00002232\n",
            "Iteration  831: Training Loss 0.00000217, Validation Loss 0.00002232\n",
            "Iteration  832: Training Loss 0.00000217, Validation Loss 0.00002232\n",
            "Iteration  833: Training Loss 0.00000217, Validation Loss 0.00002232\n",
            "Iteration  834: Training Loss 0.00000217, Validation Loss 0.00002232\n",
            "Iteration  835: Training Loss 0.00000217, Validation Loss 0.00002233\n",
            "Iteration  836: Training Loss 0.00000217, Validation Loss 0.00002233\n",
            "Iteration  837: Training Loss 0.00000217, Validation Loss 0.00002233\n",
            "Iteration  838: Training Loss 0.00000217, Validation Loss 0.00002233\n",
            "Iteration  839: Training Loss 0.00000217, Validation Loss 0.00002233\n",
            "Iteration  840: Training Loss 0.00000217, Validation Loss 0.00002234\n",
            "Iteration  841: Training Loss 0.00000217, Validation Loss 0.00002234\n",
            "Iteration  842: Training Loss 0.00000217, Validation Loss 0.00002234\n",
            "Iteration  843: Training Loss 0.00000217, Validation Loss 0.00002234\n",
            "Iteration  844: Training Loss 0.00000217, Validation Loss 0.00002234\n",
            "Iteration  845: Training Loss 0.00000217, Validation Loss 0.00002235\n",
            "Iteration  846: Training Loss 0.00000217, Validation Loss 0.00002235\n",
            "Iteration  847: Training Loss 0.00000217, Validation Loss 0.00002235\n",
            "Iteration  848: Training Loss 0.00000217, Validation Loss 0.00002235\n",
            "Iteration  849: Training Loss 0.00000217, Validation Loss 0.00002235\n",
            "Iteration  850: Training Loss 0.00000217, Validation Loss 0.00002236\n",
            "Iteration  851: Training Loss 0.00000217, Validation Loss 0.00002236\n",
            "Iteration  852: Training Loss 0.00000217, Validation Loss 0.00002236\n",
            "Iteration  853: Training Loss 0.00000217, Validation Loss 0.00002236\n",
            "Iteration  854: Training Loss 0.00000217, Validation Loss 0.00002236\n",
            "Iteration  855: Training Loss 0.00000217, Validation Loss 0.00002237\n",
            "Iteration  856: Training Loss 0.00000217, Validation Loss 0.00002237\n",
            "Iteration  857: Training Loss 0.00000217, Validation Loss 0.00002237\n",
            "Iteration  858: Training Loss 0.00000216, Validation Loss 0.00002237\n",
            "Iteration  859: Training Loss 0.00000216, Validation Loss 0.00002237\n",
            "Iteration  860: Training Loss 0.00000216, Validation Loss 0.00002237\n",
            "Iteration  861: Training Loss 0.00000216, Validation Loss 0.00002238\n",
            "Iteration  862: Training Loss 0.00000216, Validation Loss 0.00002238\n",
            "Iteration  863: Training Loss 0.00000216, Validation Loss 0.00002238\n",
            "Iteration  864: Training Loss 0.00000216, Validation Loss 0.00002238\n",
            "Iteration  865: Training Loss 0.00000216, Validation Loss 0.00002238\n",
            "Iteration  866: Training Loss 0.00000216, Validation Loss 0.00002239\n",
            "Iteration  867: Training Loss 0.00000216, Validation Loss 0.00002239\n",
            "Iteration  868: Training Loss 0.00000216, Validation Loss 0.00002239\n",
            "Iteration  869: Training Loss 0.00000216, Validation Loss 0.00002239\n",
            "Iteration  870: Training Loss 0.00000216, Validation Loss 0.00002239\n",
            "Iteration  871: Training Loss 0.00000216, Validation Loss 0.00002240\n",
            "Iteration  872: Training Loss 0.00000216, Validation Loss 0.00002240\n",
            "Iteration  873: Training Loss 0.00000216, Validation Loss 0.00002240\n",
            "Iteration  874: Training Loss 0.00000216, Validation Loss 0.00002240\n",
            "Iteration  875: Training Loss 0.00000216, Validation Loss 0.00002240\n",
            "Iteration  876: Training Loss 0.00000216, Validation Loss 0.00002240\n",
            "Iteration  877: Training Loss 0.00000216, Validation Loss 0.00002241\n",
            "Iteration  878: Training Loss 0.00000216, Validation Loss 0.00002241\n",
            "Iteration  879: Training Loss 0.00000216, Validation Loss 0.00002241\n",
            "Iteration  880: Training Loss 0.00000216, Validation Loss 0.00002241\n",
            "Iteration  881: Training Loss 0.00000216, Validation Loss 0.00002241\n",
            "Iteration  882: Training Loss 0.00000216, Validation Loss 0.00002242\n",
            "Iteration  883: Training Loss 0.00000216, Validation Loss 0.00002242\n",
            "Iteration  884: Training Loss 0.00000216, Validation Loss 0.00002242\n",
            "Iteration  885: Training Loss 0.00000216, Validation Loss 0.00002242\n",
            "Iteration  886: Training Loss 0.00000215, Validation Loss 0.00002242\n",
            "Iteration  887: Training Loss 0.00000215, Validation Loss 0.00002242\n",
            "Iteration  888: Training Loss 0.00000215, Validation Loss 0.00002243\n",
            "Iteration  889: Training Loss 0.00000215, Validation Loss 0.00002243\n",
            "Iteration  890: Training Loss 0.00000215, Validation Loss 0.00002243\n",
            "Iteration  891: Training Loss 0.00000215, Validation Loss 0.00002243\n",
            "Iteration  892: Training Loss 0.00000215, Validation Loss 0.00002243\n",
            "Iteration  893: Training Loss 0.00000215, Validation Loss 0.00002243\n",
            "Iteration  894: Training Loss 0.00000215, Validation Loss 0.00002244\n",
            "Iteration  895: Training Loss 0.00000215, Validation Loss 0.00002244\n",
            "Iteration  896: Training Loss 0.00000215, Validation Loss 0.00002244\n",
            "Iteration  897: Training Loss 0.00000215, Validation Loss 0.00002244\n",
            "Iteration  898: Training Loss 0.00000215, Validation Loss 0.00002244\n",
            "Iteration  899: Training Loss 0.00000215, Validation Loss 0.00002245\n",
            "Iteration  900: Training Loss 0.00000215, Validation Loss 0.00002245\n",
            "Iteration  901: Training Loss 0.00000215, Validation Loss 0.00002245\n",
            "Iteration  902: Training Loss 0.00000215, Validation Loss 0.00002245\n",
            "Iteration  903: Training Loss 0.00000215, Validation Loss 0.00002245\n",
            "Iteration  904: Training Loss 0.00000215, Validation Loss 0.00002245\n",
            "Iteration  905: Training Loss 0.00000215, Validation Loss 0.00002246\n",
            "Iteration  906: Training Loss 0.00000215, Validation Loss 0.00002246\n",
            "Iteration  907: Training Loss 0.00000215, Validation Loss 0.00002246\n",
            "Iteration  908: Training Loss 0.00000215, Validation Loss 0.00002246\n",
            "Iteration  909: Training Loss 0.00000215, Validation Loss 0.00002246\n",
            "Iteration  910: Training Loss 0.00000215, Validation Loss 0.00002246\n",
            "Iteration  911: Training Loss 0.00000215, Validation Loss 0.00002247\n",
            "Iteration  912: Training Loss 0.00000215, Validation Loss 0.00002247\n",
            "Iteration  913: Training Loss 0.00000215, Validation Loss 0.00002247\n",
            "Iteration  914: Training Loss 0.00000215, Validation Loss 0.00002247\n",
            "Iteration  915: Training Loss 0.00000215, Validation Loss 0.00002247\n",
            "Iteration  916: Training Loss 0.00000214, Validation Loss 0.00002247\n",
            "Iteration  917: Training Loss 0.00000214, Validation Loss 0.00002248\n",
            "Iteration  918: Training Loss 0.00000214, Validation Loss 0.00002248\n",
            "Iteration  919: Training Loss 0.00000214, Validation Loss 0.00002248\n",
            "Iteration  920: Training Loss 0.00000214, Validation Loss 0.00002248\n",
            "Iteration  921: Training Loss 0.00000214, Validation Loss 0.00002248\n",
            "Iteration  922: Training Loss 0.00000214, Validation Loss 0.00002248\n",
            "Iteration  923: Training Loss 0.00000214, Validation Loss 0.00002249\n",
            "Iteration  924: Training Loss 0.00000214, Validation Loss 0.00002249\n",
            "Iteration  925: Training Loss 0.00000214, Validation Loss 0.00002249\n",
            "Iteration  926: Training Loss 0.00000214, Validation Loss 0.00002249\n",
            "Iteration  927: Training Loss 0.00000214, Validation Loss 0.00002249\n",
            "Iteration  928: Training Loss 0.00000214, Validation Loss 0.00002249\n",
            "Iteration  929: Training Loss 0.00000214, Validation Loss 0.00002250\n",
            "Iteration  930: Training Loss 0.00000214, Validation Loss 0.00002250\n",
            "Iteration  931: Training Loss 0.00000214, Validation Loss 0.00002250\n",
            "Iteration  932: Training Loss 0.00000214, Validation Loss 0.00002250\n",
            "Iteration  933: Training Loss 0.00000214, Validation Loss 0.00002250\n",
            "Iteration  934: Training Loss 0.00000214, Validation Loss 0.00002250\n",
            "Iteration  935: Training Loss 0.00000214, Validation Loss 0.00002251\n",
            "Iteration  936: Training Loss 0.00000214, Validation Loss 0.00002251\n",
            "Iteration  937: Training Loss 0.00000214, Validation Loss 0.00002251\n",
            "Iteration  938: Training Loss 0.00000214, Validation Loss 0.00002251\n",
            "Iteration  939: Training Loss 0.00000214, Validation Loss 0.00002251\n",
            "Iteration  940: Training Loss 0.00000214, Validation Loss 0.00002251\n",
            "Iteration  941: Training Loss 0.00000214, Validation Loss 0.00002252\n",
            "Iteration  942: Training Loss 0.00000214, Validation Loss 0.00002252\n",
            "Iteration  943: Training Loss 0.00000214, Validation Loss 0.00002252\n",
            "Iteration  944: Training Loss 0.00000214, Validation Loss 0.00002252\n",
            "Iteration  945: Training Loss 0.00000214, Validation Loss 0.00002252\n",
            "Iteration  946: Training Loss 0.00000214, Validation Loss 0.00002252\n",
            "Iteration  947: Training Loss 0.00000214, Validation Loss 0.00002253\n",
            "Iteration  948: Training Loss 0.00000213, Validation Loss 0.00002253\n",
            "Iteration  949: Training Loss 0.00000213, Validation Loss 0.00002253\n",
            "Iteration  950: Training Loss 0.00000213, Validation Loss 0.00002253\n",
            "Iteration  951: Training Loss 0.00000213, Validation Loss 0.00002253\n",
            "Iteration  952: Training Loss 0.00000213, Validation Loss 0.00002253\n",
            "Iteration  953: Training Loss 0.00000213, Validation Loss 0.00002254\n",
            "Iteration  954: Training Loss 0.00000213, Validation Loss 0.00002254\n",
            "Iteration  955: Training Loss 0.00000213, Validation Loss 0.00002254\n",
            "Iteration  956: Training Loss 0.00000213, Validation Loss 0.00002254\n",
            "Iteration  957: Training Loss 0.00000213, Validation Loss 0.00002254\n",
            "Iteration  958: Training Loss 0.00000213, Validation Loss 0.00002254\n",
            "Iteration  959: Training Loss 0.00000213, Validation Loss 0.00002254\n",
            "Iteration  960: Training Loss 0.00000213, Validation Loss 0.00002255\n",
            "Iteration  961: Training Loss 0.00000213, Validation Loss 0.00002255\n",
            "Iteration  962: Training Loss 0.00000213, Validation Loss 0.00002255\n",
            "Iteration  963: Training Loss 0.00000213, Validation Loss 0.00002255\n",
            "Iteration  964: Training Loss 0.00000213, Validation Loss 0.00002255\n",
            "Iteration  965: Training Loss 0.00000213, Validation Loss 0.00002255\n",
            "Iteration  966: Training Loss 0.00000213, Validation Loss 0.00002256\n",
            "Iteration  967: Training Loss 0.00000213, Validation Loss 0.00002256\n",
            "Iteration  968: Training Loss 0.00000213, Validation Loss 0.00002256\n",
            "Iteration  969: Training Loss 0.00000213, Validation Loss 0.00002256\n",
            "Iteration  970: Training Loss 0.00000213, Validation Loss 0.00002256\n",
            "Iteration  971: Training Loss 0.00000213, Validation Loss 0.00002256\n",
            "Iteration  972: Training Loss 0.00000213, Validation Loss 0.00002256\n",
            "Iteration  973: Training Loss 0.00000213, Validation Loss 0.00002257\n",
            "Iteration  974: Training Loss 0.00000213, Validation Loss 0.00002257\n",
            "Iteration  975: Training Loss 0.00000213, Validation Loss 0.00002257\n",
            "Iteration  976: Training Loss 0.00000213, Validation Loss 0.00002257\n",
            "Iteration  977: Training Loss 0.00000213, Validation Loss 0.00002257\n",
            "Iteration  978: Training Loss 0.00000213, Validation Loss 0.00002257\n",
            "Iteration  979: Training Loss 0.00000213, Validation Loss 0.00002257\n",
            "Iteration  980: Training Loss 0.00000213, Validation Loss 0.00002258\n",
            "Iteration  981: Training Loss 0.00000213, Validation Loss 0.00002258\n",
            "Iteration  982: Training Loss 0.00000212, Validation Loss 0.00002258\n",
            "Iteration  983: Training Loss 0.00000212, Validation Loss 0.00002258\n",
            "Iteration  984: Training Loss 0.00000212, Validation Loss 0.00002258\n",
            "Iteration  985: Training Loss 0.00000212, Validation Loss 0.00002258\n",
            "Iteration  986: Training Loss 0.00000212, Validation Loss 0.00002259\n",
            "Iteration  987: Training Loss 0.00000212, Validation Loss 0.00002259\n",
            "Iteration  988: Training Loss 0.00000212, Validation Loss 0.00002259\n",
            "Iteration  989: Training Loss 0.00000212, Validation Loss 0.00002259\n",
            "Iteration  990: Training Loss 0.00000212, Validation Loss 0.00002259\n",
            "Iteration  991: Training Loss 0.00000212, Validation Loss 0.00002259\n",
            "Iteration  992: Training Loss 0.00000212, Validation Loss 0.00002259\n",
            "Iteration  993: Training Loss 0.00000212, Validation Loss 0.00002260\n",
            "Iteration  994: Training Loss 0.00000212, Validation Loss 0.00002260\n",
            "Iteration  995: Training Loss 0.00000212, Validation Loss 0.00002260\n",
            "Iteration  996: Training Loss 0.00000212, Validation Loss 0.00002260\n",
            "Iteration  997: Training Loss 0.00000212, Validation Loss 0.00002260\n",
            "Iteration  998: Training Loss 0.00000212, Validation Loss 0.00002260\n",
            "Iteration  999: Training Loss 0.00000212, Validation Loss 0.00002260\n",
            "Iteration 1000: Training Loss 0.00000212, Validation Loss 0.00002261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(range(1, num_iterations + 1), MSE_train_points, label='Training Loss')\n",
        "plt.plot(range(1, num_iterations + 1), MSE_val_points, label='Validation Loss')\n",
        "\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f_CSVo1x0Reh",
        "outputId": "8921b735-0bcb-4d96-d32d-866fbe8b33a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}